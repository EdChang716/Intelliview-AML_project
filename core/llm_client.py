# core/llm_client.py
import os
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai as genai

BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv(BASE_DIR / ".env")

# 1. åˆå§‹åŒ– OpenAI Client
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    # å¦‚æœé€£ OpenAIéƒ½æ²’æœ‰ï¼Œé‚£çœŸçš„æ²’æ•‘äº†ï¼Œç›´æ¥å™´éŒ¯
    raise RuntimeError("OPENAI_API_KEY not found in environment or .env")

openai_client = OpenAI(api_key=openai_api_key)

# 2. åˆå§‹åŒ– Gemini Client (å…è¨±å¤±æ•—)
gemini_api_key = os.getenv("GEMINI_API_KEY")
if gemini_api_key:
    genai.configure(api_key=gemini_api_key)
else:
    print("Warning: GEMINI_API_KEY not found in .env. Will use OpenAI fallback.")


def ask_llm(
    messages: list[dict],
    provider: str = "openai",
    model: str = None,
    temperature: float = 0.7,
    max_tokens: int = 1000
) -> str:
    """
    çµ±ä¸€çš„ LLM å‘¼å«æ¥å£ã€‚
    ğŸ”¥ å®‰å…¨æ©Ÿåˆ¶ï¼šå¦‚æœ Gemini å‘¼å«å¤±æ•— (ä¾‹å¦‚ Key Leaked / 403 / ç¶²è·¯éŒ¯èª¤)ï¼Œ
    æœƒè‡ªå‹•åˆ‡æ›å› OpenAIï¼Œä¿è­‰ç¨‹å¼ä¸å´©æ½°ã€‚
    """

    # å®šç¾©ä¸€å€‹å…§éƒ¨å‡½å¼ä¾†å‘¼å« OpenAIï¼Œæ–¹ä¾¿ç­‰ä¸€ä¸‹é‡è¤‡ä½¿ç”¨
    def call_openai():
        # å¦‚æœæ˜¯ fallback éä¾†çš„ï¼Œæˆ‘å€‘é€šå¸¸å¼·åˆ¶ç”¨ä¾¿å®œå¥½ç”¨çš„ gpt-4o-mini
        target_model = "gpt-4o-mini" 
        try:
            response = openai_client.chat.completions.create(
                model=target_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"Error calling OpenAI (Fallback failed): {str(e)}"

    # --- ä¸»é‚è¼¯é–‹å§‹ ---
    try:
        # å¦‚æœæŒ‡å®šè¦ç”¨ Geminiï¼Œä¸”æˆ‘å€‘æœ‰ Key
        if provider == "gemini":
            if not gemini_api_key:
                print("âš ï¸ No Gemini Key found. Falling back to OpenAI.")
                return call_openai()
            
            try:
                # å˜—è©¦å‘¼å« Gemini
                target_model = model or "gemini-2.5-flash-lite"
                gemini_model = genai.GenerativeModel(target_model)
                
                # Gemini 1.5 æ¥å—ç´”æ–‡å­— Promptï¼Œæˆ‘å€‘æŠŠå°è©±æ¥èµ·ä¾†
                full_prompt = "\n\n".join([m["content"] for m in messages])
                
                response = gemini_model.generate_content(
                    full_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=max_tokens
                    )
                )
                return response.text.strip()
            
            except Exception as gemini_error:
                # æ•æ‰æ‰€æœ‰ Gemini çš„éŒ¯èª¤ (åŒ…å« 403 Key Leaked)
                print(f"ğŸ”´ Gemini Error: {gemini_error}")
                print("ğŸ”„ Automatically switching to OpenAI...")
                return call_openai()

        else:
            # åŸæœ¬å°±æŒ‡å®šç”¨ OpenAI
            return call_openai()

    except Exception as e:
        return f"System Error: {str(e)}"

# --- å‘ä¸‹ç›¸å®¹è¨­å®š (Backward Compatibility) ---
# é€™ä¸€è¡Œæ˜¯ç‚ºäº†è®“å…¶ä»–é‚„æ²’æ”¹å¯«çš„æª”æ¡ˆ (å¦‚ core/answers.py) 
# å¯ä»¥ç¹¼çºŒç”¨ `from .llm_client import client` è€Œä¸å ±éŒ¯
client = openai_client
