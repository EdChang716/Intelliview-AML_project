from __future__ import annotations

import datetime
import json as _json
import random
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional
from pathlib import Path
import subprocess
from fastapi import HTTPException

from .retrieval import retrieve_bullets_for_profile, load_resume_entries_and_embs
from .llm_client import client
from core.answers import evaluate_answer
from .transcription import transcribe_media_with_segments
from .profiles import load_job_profiles
from .questions import (
    call_llm_for_question,         # ç”¨ JD + LLM ç”Ÿ technical / auto / case é¡Œ
    call_llm_for_project_question  # ç”¨ JD + resume entry ç”Ÿ project deep dive é¡Œ
)
from core.video_features import extract_video_features

# ---- è·¯å¾‘è¨­å®š ----

BASE_DIR = Path(__file__).resolve().parents[1]
USER_DATA_DIR = BASE_DIR / "user_data"

MOCK_BASE_DIR = USER_DATA_DIR / "mock"
MOCK_SESSIONS_DIR = MOCK_BASE_DIR / "sessions"
MOCK_MEDIA_DIR = MOCK_BASE_DIR / "media"
MOCK_RESULTS_DIR = MOCK_BASE_DIR / "results"

for d in [MOCK_SESSIONS_DIR, MOCK_MEDIA_DIR, MOCK_RESULTS_DIR]:
    d.mkdir(parents=True, exist_ok=True)


# ============================================================
#  Question è¨­è¨ˆï¼šé¡Œæ•¸ / æ™‚é–“ä¼°è¨ˆ & é¡Œå‹è¦åŠƒ
# ============================================================

def _estimate_questions_for_time(minutes: int) -> int:
    """
    æ™‚é–“åˆ¶ mock çš„æ™‚å€™ï¼Œç²—ä¼°ä¸€ä¸‹é¡Œæ•¸ã€‚
    ï¼ˆåªæ˜¯ç”¨ä¾†å…ˆæ’å‡º question_planï¼ŒçœŸæ­£çµæŸæ™‚é–“å‰ç«¯å¯ä»¥å†ç”¨ timer æ§åˆ¶ï¼‰
    """
    # å…ˆçµ¦ä¸€å€‹æ¯”è¼ƒå¤§çš„ upper boundï¼Œå¯¦éš›çµæŸç”±å‰ç«¯ countdown æ§åˆ¶
    return 30  # TODO: ä¹‹å¾Œå¯ä»¥ä¾ minutes æ¯”è¼ƒç²¾æº–ä¼°è¨ˆ


def _build_question_plan(
    length_type: str,
    num_questions: Optional[int],
    time_limit: Optional[int],
) -> List[Dict[str, Any]]:
    """
    æ–°ç‰ˆ question_planï¼š

    - 1 é¡Œï¼šintro
    - 2 é¡Œï¼šintro â†’ project
    - 3 é¡Œï¼šintro â†’ project â†’ case
    - 4 é¡Œï¼šintro â†’ project â†’ case â†’ behavioral
    - â‰¥5 é¡Œï¼ˆæ­£å¸¸å®Œæ•´ flowï¼‰ï¼š
        Q0: intro
        Q1: project deep diveï¼ˆæ ¹æ“š JD + resume æŒ‘ projectï¼‰
        Q2: technicalï¼ˆJD-basedï¼‰
        Q3: case reasoningï¼ˆè·Ÿ JD å°é½Šçš„ data/ML/product caseï¼‰
        ä¸­é–“ï¼štechnical / auto é¡Œ
        æœ€å¾Œ 1â€“2 é¡Œï¼šbehavioral æ”¶å°¾
    """
    if length_type == "questions":
        total_slots = max(1, num_questions or 5)
    else:
        total_slots = max(1, _estimate_questions_for_time(time_limit or 30))

    plan: List[Dict[str, Any]] = []

    # --- å°é¡Œæ•¸çš„ç‰¹ä¾‹ ---
    if total_slots == 1:
        plan.append({"index": 0, "type": "intro"})
        return plan

    if total_slots == 2:
        plan.append({"index": 0, "type": "intro"})
        plan.append({"index": 1, "type": "project"})
        return plan

    if total_slots == 3:
        plan.append({"index": 0, "type": "intro"})
        plan.append({"index": 1, "type": "project"})
        plan.append({"index": 2, "type": "case"})
        return plan

    if total_slots == 4:
        # ç›¡é‡ç¶­æŒä½ æƒ³è¦çš„ flowï¼šintro â†’ project â†’ case â†’ behavioral
        plan.append({"index": 0, "type": "intro"})
        plan.append({"index": 1, "type": "project"})
        plan.append({"index": 2, "type": "case"})
        plan.append({"index": 3, "type": "behavioral"})
        return plan

    # --- general case: total_slots >= 5 ---
    plan.append({"index": 0, "type": "intro"})     # Q0: intro
    plan.append({"index": 1, "type": "project"})   # Q1: project deep dive
    plan.append({"index": 2, "type": "technical"}) # Q2: technical
    plan.append({"index": 3, "type": "case"})      # Q3: case reasoning

    # æˆ‘å€‘ä¿ç•™æœ€å¾Œå…©é¡Œçµ¦ behavioralï¼Œå¦‚æœåªæœ‰ä¸€é¡Œç©ºé–“å°±ç•™æœ€å¾Œä¸€é¡Œ
    last_behavioral_start = max(4, total_slots - 2)

    # ä¸­é–“ technical / auto é¡Œï¼ˆåœ¨ case ä¹‹å¾Œã€behavioral ä¹‹å‰ï¼‰
    idx = 4
    while idx < last_behavioral_start:
        q_type = "technical" if random.random() < 0.7 else "auto"
        plan.append({"index": idx, "type": q_type})
        idx += 1

    # æœ€å¾Œ 1â€“2 é¡Œï¼šbehavioral
    while idx < total_slots:
        plan.append({"index": idx, "type": "behavioral"})
        idx += 1

    # ä¿éšª re-index
    for i, spec in enumerate(plan):
        spec["index"] = i

    return plan


# ============================================================
#  Session ç®¡ç† & project deep dive entry é¸æ“‡
# ============================================================

def _load_profile_jd_for_questions(profile_id: Optional[str]) -> str:
    """
    çµ¦ technical / auto / project deep dive / case å‡ºé¡Œç”¨ï¼Œå¾ job_profiles.json è£¡æ‰¾åˆ°å°æ‡‰ profile çš„ JD æ–‡å­—ã€‚
    """
    if not profile_id:
        return ""

    try:
        profiles = load_job_profiles()  # list[dict]
    except Exception:
        return ""

    profile = next(
        (p for p in profiles if p.get("profile_id") == profile_id),
        None,
    )
    if not profile:
        return ""

    return (
        profile.get("jd_text")
        or profile.get("jd")
        or profile.get("job_description")
        or ""
    )


def _pick_primary_project_entry(profile_id: str, resume_id: str) -> Optional[str]:
    """
    æ··åˆç­–ç•¥ï¼š
      1) ç”¨ JD ç•¶ queryï¼Œretrieve å‰ 10 å€‹æœ€ç›¸é—œ bullet
      2) å¾é€™ 10 é¡† bullet è£¡æ‰¾å‡ºæœ€å¤š 3 å€‹ entry_key (section||entry) ç•¶å€™é¸
      3) è®€å–æ•´ä»½å±¥æ­·ï¼ŒæŠŠé€™äº›å€™é¸ entry åº•ä¸‹çš„æ‰€æœ‰ bullet æ•´ç†å‡ºä¾†
      4) ä¸Ÿçµ¦ LLMï¼Œè«‹å®ƒæ ¹æ“š JD é¸å‡ºæœ€é©åˆåš primary deep dive çš„ entry_key

    å›å‚³ï¼šentry_key (ä¾‹å¦‚ "EXPERIENCE||CAYIN Technology â€“ ML Intern")
    """

    # 1) è®€ JD
    jd_text = _load_profile_jd_for_questions(profile_id)
    if not jd_text.strip():
        return None

    # 2) ç”¨ JD åšä¸€æ¬¡ RAGï¼Œåªè¦ top_k=10ï¼ˆå‰ 10 é¡†æœ€ç›¸é—œ bulletï¼‰
    try:
        top_bullets = retrieve_bullets_for_profile(profile_id, jd_text, top_k=10)
    except Exception as e:
        print("[mock] _pick_primary_project_entry retrieve error:", e)
        return None

    if not top_bullets:
        return None

    # 3) å¾å‰ 10 é¡† bullet è£¡çµ±è¨ˆ entry_keyï¼ˆæœ€å¤šå– 3 å€‹ï¼‰
    stats: Dict[str, Dict[str, Any]] = {}
    for rank, b in enumerate(top_bullets):
        entry = b.get("entry")
        section = b.get("section") or "EXPERIENCE"
        if not entry:
            continue
        entry_key = f"{section}||{entry}"
        if entry_key not in stats:
            stats[entry_key] = {"count": 0, "best_rank": rank}
        stats[entry_key]["count"] += 1
        stats[entry_key]["best_rank"] = min(stats[entry_key]["best_rank"], rank)

    if not stats:
        return None

    # æŒ‰ã€Œå‡ºç¾æ¬¡æ•¸å¤šã€å„ªå…ˆï¼Œå…¶æ¬¡ã€Œbest_rank é å‰ã€æ’åº
    ranked = sorted(
        stats.items(),
        key=lambda kv: (-kv[1]["count"], kv[1]["best_rank"])
    )

    # åªå–æœ€å¤šå‰ 3 å€‹ entry_key ç•¶å€™é¸ï¼ˆå¦‚æœæœ¬ä¾†å°±åªæœ‰ 1 æˆ– 2 å€‹å°±ç…§å¯¦éš›æ•¸é‡ï¼‰
    top_k = min(3, len(ranked))
    candidate_entry_keys = [rk[0] for rk in ranked[:top_k]]

    if not candidate_entry_keys:
        return None

    # 4) è¼‰å…¥æ•´ä»½å±¥æ­·çš„ bulletï¼Œæ•´ç†å‡ºæ¯å€‹ candidate entry_key å°æ‡‰çš„å®Œæ•´ bullets
    try:
        all_entries, _ = load_resume_entries_and_embs(resume_id)
    except Exception as e:
        print("[mock] load_resume_entries_and_embs error:", e)
        all_entries = []

    # å»ºä¸€å€‹ mapping: entry_key -> {"title": ..., "bullets": [..]}
    candidate_projects: Dict[str, Dict[str, Any]] = {}
    for entry_key in candidate_entry_keys:
        candidate_projects[entry_key] = {
            "title": entry_key.split("||", 1)[1] if "||" in entry_key else entry_key,
            "bullets": [],
        }

    for e in all_entries:
        section = e.get("section") or "EXPERIENCE"
        entry = e.get("entry") or ""
        if not entry:
            continue
        key = f"{section}||{entry}"
        if key not in candidate_projects:
            continue
        text = e.get("text") or ""
        if text.strip():
            candidate_projects[key]["bullets"].append(text.strip())

    # æŠŠæ²’æœ‰ä»»ä½• bullet çš„å€™é¸åˆªæ‰ï¼ˆç†è«–ä¸Šä¸å¤ªæœƒç™¼ç”Ÿï¼‰
    candidate_entry_keys = [
        k for k in candidate_entry_keys
        if candidate_projects.get(k, {}).get("bullets")
    ]
    if not candidate_entry_keys:
        return None

    # 5) ç”¨ LLM åœ¨é€™å¹¾å€‹å€™é¸ä¸­é¸å‡ºã€Œæœ€é©åˆ deep dive çš„ projectã€
    projects_block_lines = []
    for entry_key in candidate_entry_keys:
        proj = candidate_projects[entry_key]
        title = proj["title"]
        bullets = proj["bullets"]
        projects_block_lines.append(f"ID: {entry_key}\nTitle: {title}\nBullets:")
        for bt in bullets:
            projects_block_lines.append(f"- {bt}")
        projects_block_lines.append("")  # ç©ºè¡Œåˆ†éš”

    projects_block = "\n".join(projects_block_lines)

    system_msg = (
        "You are a hiring manager preparing for a data / ML interview.\n"
        "Given a job description and several projects from the candidate's resume, "
        "choose ONE project that is the best primary deep-dive topic for this interview."
    )

    user_msg = (
        "Job description:\n"
        f"{jd_text}\n\n"
        "Here are candidate projects from the resume. Each project has an ID, a title, and its bullets:\n\n"
        f"{projects_block}\n"
        "Your task:\n"
        "- Choose EXACTLY ONE project that is the best fit to deep-dive on for this job.\n"
        "- Prefer projects that (1) match the tools and responsibilities in the job, "
        "(2) show end-to-end ownership, and (3) have clear impact or measurable results.\n\n"
        "Return ONLY the ID of the chosen project (exactly one of the IDs shown above), with no explanation."
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.0,
        )
        raw_choice = (resp.choices[0].message.content or "").strip()
    except Exception as e:
        print("[mock] LLM choose primary project error:", e)
        raw_choice = ""

    # ç¢ºä¿å›å‚³çš„æ˜¯å€™é¸è£¡çš„ä¸€å€‹ï¼›è‹¥ LLM å›å‚³æ€ªæ€ªçš„å°± fallback ç¬¬ä¸€å€‹
    chosen = None
    for key in candidate_entry_keys:
        if key == raw_choice:
            chosen = key
            break
    if chosen is None:
        raw_norm = raw_choice.strip()
        for key in candidate_entry_keys:
            if key.strip() == raw_norm:
                chosen = key
                break

    if chosen is None:
        chosen = candidate_entry_keys[0]

    print("[mock] picked primary_project_entry_key (hybrid) =", chosen)
    return chosen


def create_mock_session(
    profile_id: str,
    resume_id: str,
    mode: str,
    length_type: str,
    hint_level: str,
    num_questions: Optional[int] = None,
    time_limit: Optional[int] = None,
    interviewer_profile: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    å¾ mock settings é å»ºç«‹ä¸€å€‹æ–°çš„ mock sessionï¼Œå­˜æˆ jsonã€‚

    å›å‚³ session dictï¼Œè£¡é¢æœƒæœ‰ï¼š
    - session_id
    - num_questionsï¼ˆå¯¦éš›æœƒè¢«å•åˆ°çš„ slot æ•¸ï¼ŒåŒ…æ‹¬ follow-upsï¼‰
    - interviewer_profileï¼šé¢è©¦å®˜ persona è¨­å®š
    """
    session_id = str(uuid.uuid4())

    question_plan = _build_question_plan(
        length_type=length_type,
        num_questions=num_questions,
        time_limit=time_limit,
    )

    now = datetime.datetime.utcnow().isoformat()

    # å•é¡Œæ¨¡å¼æ‰å›ºå®š main é¡Œæ•¸ï¼›æ™‚é–“æ¨¡å¼äº¤çµ¦å‰ç«¯å€’æ•¸è¨ˆæ™‚æ§åˆ¶
    if length_type == "questions":
        num_questions_planned = len(question_plan)
    else:
        num_questions_planned = None  # ä¸ç”¨é€™å€‹æ¬„ä½ä¾†æˆªæ–·

    # å…ˆå»ºç«‹åŸºæœ¬ session
    session: Dict[str, Any] = {
        "session_id": session_id,
        "profile_id": profile_id,
        "resume_id": resume_id,
        "mode": mode,
        "length_type": length_type,
        "hint_level": hint_level,
        "num_questions": num_questions_planned,
        "time_limit": time_limit,          # é€™å€‹ç­‰ç­‰è¦ä¸Ÿçµ¦å‰ç«¯
        "question_plan": question_plan,
        "created_at": now,
        "completed": False,
        "used_question_ids": [],
        "used_question_slugs": [],
        # â˜… NEW: è¨˜éŒ„å•éçš„é¡Œç›®æ–‡å­—ï¼Œè®“ technical / auto / case é¡Œå¯ä»¥é¿å…é‡è¤‡
        "used_question_texts": [],
        # â˜… NEW: interviewer persona
        "interviewer_profile": interviewer_profile or {},
    }

    # â˜… NEW: å»ºç«‹ project deep dive ç”¨çš„ primary_project_entry_key
    primary_project_entry_key = _pick_primary_project_entry(profile_id, resume_id)
    if primary_project_entry_key:
        session["primary_project_entry_key"] = primary_project_entry_key

    path = MOCK_SESSIONS_DIR / f"{session_id}.json"
    with path.open("w", encoding="utf-8") as f:
        _json.dump(session, f, ensure_ascii=False, indent=2)

    return session


def load_mock_session(session_id: str) -> Dict[str, Any]:
    path = MOCK_SESSIONS_DIR / f"{session_id}.json"
    if not path.exists():
        raise FileNotFoundError(f"mock session not found: {session_id}")
    with path.open("r", encoding="utf-8") as f:
        return _json.load(f)


def save_mock_session(session: Dict[str, Any]) -> None:
    session_id = session["session_id"]
    path = MOCK_SESSIONS_DIR / f"{session_id}.json"
    with path.open("w", encoding="utf-8") as f:
        _json.dump(session, f, ensure_ascii=False, indent=2)


# ============================================================
#  é¡Œåº«ï¼šbehavioral / auto / follow-upï¼ˆtechnical/auto/case ç”¨ LLM + JDï¼‰
# ============================================================

INTRO_QUESTION = {
    "question_id": "intro_q1",
    "slug": "intro_self_intro",
    "text": (
        "Hi, thanks for taking the time today. "
        "To start, could you give me a brief introduction of yourself?"
    ),
    "tag": "Intro Â· Warm-up",
    "type": "intro",
}

# â˜… Behavioral é¡Œåº«
BEHAVIORAL_BANK: List[Dict[str, Any]] = [
    {
        "id": "beh_lead_team",
        "slug": "leadership_lead_team",
        "category": "leadership",
        "text": "Tell me about a time you had to take the lead on a project.",
        "tag": "Behavioral Â· Leadership",
    },
    {
        "id": "beh_ownership",
        "slug": "ownership_end_to_end",
        "category": "ownership",
        "text": "Describe a situation where you took end-to-end ownership of a problem or project.",
        "tag": "Behavioral Â· Ownership",
    },
    {
        "id": "beh_conflict_teammate",
        "slug": "conflict_teammate",
        "category": "conflict",
        "text": "Describe a time when you disagreed with a teammate. How did you handle it?",
        "tag": "Behavioral Â· Conflict",
    },
    {
        "id": "beh_conflict_stakeholder",
        "slug": "conflict_stakeholder",
        "category": "conflict",
        "text": "Tell me about a time you had to push back on a stakeholder or manager.",
        "tag": "Behavioral Â· Stakeholder management",
    },
    {
        "id": "beh_failure",
        "slug": "failure_learn",
        "category": "failure",
        "text": "Tell me about a time you failed at something important. What did you learn?",
        "tag": "Behavioral Â· Failure",
    },
    {
        "id": "beh_ambiguity",
        "slug": "ambiguity_unstructured",
        "category": "ambiguity",
        "text": "Describe a time you had to work through an ambiguous or poorly defined problem.",
        "tag": "Behavioral Â· Ambiguity",
    },
    {
        "id": "beh_teamwork",
        "slug": "teamwork_collaboration",
        "category": "teamwork",
        "text": "Tell me about a time you worked closely with others to achieve a goal.",
        "tag": "Behavioral Â· Teamwork",
    },
    {
        "id": "beh_time_pressure",
        "slug": "time_pressure_deadlines",
        "category": "time_management",
        "text": "Give me an example of a time you were under a lot of time pressure. How did you prioritize and execute?",
        "tag": "Behavioral Â· Time management",
    },
    {
        "id": "beh_feedback",
        "slug": "feedback_receive",
        "category": "feedback",
        "text": "Tell me about a time you received critical feedback. What did you do afterward?",
        "tag": "Behavioral Â· Feedback",
    },
    {
        "id": "beh_communication",
        "slug": "communication_non_technical",
        "category": "communication",
        "text": "Describe a time you had to explain a complex or technical topic to a non-technical audience.",
        "tag": "Behavioral Â· Communication",
    },
]

# auto bank ä¿ç•™ä½œç‚º fallbackï¼ˆç•¶ LLM / JD å‡ºå•é¡Œæ™‚å†ç”¨ï¼‰
AUTO_BANK: List[Dict[str, Any]] = [
    {
        "id": "auto_project_favorite",
        "slug": "project_favorite_ds",
        "category": "project",
        "text": "Walk me through one of your favorite data or ML projects on your resume.",
        "tag": "Project Â· Deep dive",
    },
    {
        "id": "auto_project_challenging",
        "slug": "project_most_challenging",
        "category": "project",
        "text": "Tell me about the most technically challenging project youâ€™ve worked on.",
        "tag": "Project Â· Challenge",
    },
    {
        "id": "auto_impact",
        "slug": "impact_business",
        "category": "impact",
        "text": "Give me an example of a project where your work had a clear impact on business or users.",
        "tag": "Impact Â· Results",
    },
    {
        "id": "auto_metrics",
        "slug": "metrics_evaluation",
        "category": "metrics",
        "text": "Tell me about a time you had to define or choose metrics to evaluate your solution.",
        "tag": "Analytics Â· Metrics",
    },
]

FOLLOWUP_BANK: List[Dict[str, Any]] = [
    {
        "id": "fu_deeper_challenge",
        "slug": "followup_challenge",
        "text": "What was the most challenging part of that situation, and how did you handle it in the moment?",
    },
    {
        "id": "fu_alt_decision",
        "slug": "followup_alternative",
        "text": "If you faced a similar situation again, what would you do differently?",
        "tag": "Follow-up Â· Reflection",
    },
    {
        "id": "fu_stakeholder",
        "slug": "followup_stakeholder",
        "text": "How did the other people involved react, and how did you manage those reactions?",
        "tag": "Follow-up Â· Stakeholders",
    },
    {
        "id": "fu_impact_details",
        "slug": "followup_impact_details",
        "text": "Can you go a bit deeper into the impact? How did you know your approach was successful?",
        "tag": "Follow-up Â· Impact",
    },
]


# ============================================================
#  Interviewer persona builder
# ============================================================

def _build_interviewer_persona(session: Dict[str, Any]) -> str:
    """
    å¾ session['interviewer_profile'] çµ„ä¸€æ®µç°¡çŸ­ persona æè¿°ï¼Œ
    çµ¦ LLM å‡ºé¡Œ / follow-up / reaction ç”¨ã€‚
    """
    profile = session.get("interviewer_profile") or {}

    gender = profile.get("gender", "auto")
    role_code = profile.get("role", "senior_engineer")
    role_custom = (profile.get("role_custom") or "").strip()
    style_preset = profile.get("style_preset", "balanced")
    style_custom = (profile.get("style_custom") or "").strip()
    extra = (profile.get("extra_notes") or "").strip()

    # Role æè¿°
    ROLE_LABELS = {
        "senior_engineer": "a senior engineer on the team, focusing on technical depth and collaboration.",
        "hiring_manager": "the hiring manager, balancing technical depth with team fit, ownership, and impact.",
        "recruiter": "a recruiter or HR partner, focusing on communication, motivation, and culture fit.",
        "peer_teammate": "a future teammate, curious about how this candidate works with others and solves problems day-to-day.",
        "executive": "a director or VP, focusing on high-level impact, business value, and alignment with the orgâ€™s goals.",
    }

    if role_code == "custom" and role_custom:
        role_sentence = role_custom
    else:
        role_sentence = ROLE_LABELS.get(
            role_code,
            "a realistic interviewer for this role.",
        )

    # Style æè¿°
    STYLE_DESCRIPTIONS = {
        "balanced": "Your style is balanced: neutral but probing, professional, and fair.",
        "supportive": "Your style is supportive: encouraging, patient, and helping the candidate feel comfortable while still asking thoughtful questions.",
        "direct": "Your style is direct: concise, straightforward, and to the point, but not rude.",
        "challenging": "Your style is challenging: you push on weak spots and ask tough questions, but remain professional.",
        "high_pressure": "Your style is high-pressure: you move quickly, occasionally skeptical, and simulate a demanding interview, while still being fair.",
    }

    if style_preset == "custom" and style_custom:
        style_sentence = style_custom
    else:
        style_sentence = STYLE_DESCRIPTIONS.get(
            style_preset,
            "You keep a realistic but fair interview tone.",
        )

    # Gender / voice åªç•¶æˆæ°›åœï¼Œä¸å½±éŸ¿å…§å®¹
    gender_bits = []
    if gender == "male":
        gender_bits.append("You sound like a male interviewer.")
    elif gender == "female":
        gender_bits.append("You sound like a female interviewer.")
    elif gender == "neutral":
        gender_bits.append("Your voice and style feel gender-neutral and professional.")

    if extra:
        gender_bits.append(f"Additional notes about your style: {extra}")

    lines = [
        "You are interviewing a candidate for this role.",
        f"Your interviewer persona: {role_sentence}",
        style_sentence,
    ]
    if gender_bits:
        lines.extend(gender_bits)

    return "\n".join(lines)


# ============================================================
#  é¡Œç›®é¸å–ï¼šé¿å…é‡è¤‡ & follow-up
# ============================================================

def _pick_from_bank(
    bank: List[Dict[str, Any]],
    used_ids: set[str],
    used_slugs: set[str],
    category: Optional[str] = None,
) -> Dict[str, Any]:
    """
    å¾é¡Œåº«è£¡æŒ‘ä¸€é¡Œï¼Œç›¡é‡é¿é–‹å·²ä½¿ç”¨çš„ slugï¼ˆé¿å…èªæ„è¿‘ä¼¼çš„é‡è¤‡ï¼‰ã€‚
    - å…ˆæ‰¾ã€ŒåŒ category ä¸” slug æœªä½¿ç”¨ã€çš„
    - è‹¥æ‰¾ä¸åˆ°ï¼Œå†é€€è€Œæ±‚å…¶æ¬¡æ‰¾ã€ŒåŒ category ä»»æ„é¡Œã€
    - è‹¥é‚„æ˜¯æ²’æœ‰ï¼Œå†å¾æ•´å€‹ bank ä»»æ„é¸
    """
    candidates = [
        q for q in bank
        if (category is None or q.get("category") == category)
        and q["slug"] not in used_slugs
    ]
    if not candidates and category is not None:
        candidates = [q for q in bank if q.get("category") == category]
    if not candidates:
        candidates = bank[:]  # æœ€å¾Œä¿åº•ï¼šéš¨æ©Ÿä¸€é¡Œï¼Œä½†ä»é¿å…é‡è¤‡ id
    random.shuffle(candidates)
    for q in candidates:
        if q["id"] not in used_ids:
            return q
    # è¬ä¸€å…¨éƒ¨éƒ½ç”¨éï¼Œå°±çœŸçš„éš¨ä¾¿é¸ä¸€é¡Œ
    return random.choice(bank)


def _generate_non_intro_question(
    session: Dict[str, Any],
    spec: Dict[str, Any],
) -> Dict[str, Any]:
    """
    å›å‚³:
    {
      "question_id": "...",
      "question": "...",
      "tag": "...",
      "hints": {...},
      "entry_key": "...",   # åªæœ‰ project deep dive æœƒæœ‰ï¼Œå…¶å®ƒ type å¯çœç•¥
    }
    """
    hint_level = session.get("hint_level", "standard")
    used_ids = set(session.get("used_question_ids", []))
    used_slugs = set(session.get("used_question_slugs", []))
    used_texts = set(session.get("used_question_texts", []))

    q_type = spec["type"]
    profile_id = session.get("profile_id")

    # interviewer personaï¼šçµ¦ LLM ç”¨
    persona_text = _build_interviewer_persona(session)

    def _attach_persona_to_jd(jd_text: str) -> str:
        """
        ä¸æ”¹ call_llm_for_question çš„ interfaceï¼Œ
        æ”¹æˆæŠŠ persona prepend åˆ° jd_text å‰é¢ä¸€èµ·ä¸Ÿé€²å»ã€‚
        """
        jd_text = jd_text or ""
        if not persona_text.strip():
            return jd_text
        return (
            "Interviewer persona:\n"
            f"{persona_text}\n\n"
            "Job description and context:\n"
            f"{jd_text}"
        )

    text = ""
    tag = ""
    question_id = ""
    entry_key: Optional[str] = None
    hints: Optional[Dict[str, Any]] = None

    if q_type == "behavioral":
        base = _pick_from_bank(
            BEHAVIORAL_BANK,
            used_ids,
            used_slugs,
            category=None,  # ä¹‹å¾Œå¯ä»¥çœ‹éœ€è¦è®“ä½ é¸æŸä¸€é¡
        )
        question_id = base["id"]
        text = base["text"]
        tag = base["tag"]
        entry_key = None
        hints = _build_hints_for_generic(hint_level)

    elif q_type == "auto":
        # auto é¡Œï¼šJD-based LLM å‡ºé¡Œï¼Œfallback æ‰ç”¨ AUTO_BANK
        jd_text = _load_profile_jd_for_questions(profile_id)
        jd_for_llm = _attach_persona_to_jd(jd_text)
        if jd_for_llm.strip():
            try:
                question_text = call_llm_for_question(
                    jd_text=jd_for_llm,
                    mode="auto",
                    avoid=used_texts,
                )
                text = (question_text or "").strip()
            except Exception as e:
                print("[mock auto] LLM error:", e)

        if text:
            question_id = f"auto_{spec['index']}_{uuid.uuid4().hex[:8]}"
            tag = "General Â· JD-based"
        else:
            # fallback
            base = _pick_from_bank(AUTO_BANK, used_ids, used_slugs, category=None)
            question_id = base["id"]
            text = base["text"]
            tag = base["tag"]
        entry_key = None
        hints = _build_hints_for_generic(hint_level)

    elif q_type == "technical":
        # technical é¡Œç›®ç”¨ JD + LLM å‹•æ…‹ç”¢ç”Ÿ
        jd_text = _load_profile_jd_for_questions(profile_id)
        jd_for_llm = _attach_persona_to_jd(jd_text)
        print("[mock technical] profile_id=", profile_id, "JD length=", len(jd_text))

        if jd_for_llm.strip():
            try:
                question_text = call_llm_for_question(
                    jd_text=jd_for_llm,
                    mode="technical",
                    avoid=used_texts,
                )
                text = (question_text or "").strip()
            except Exception as e:
                print("[mock technical] LLM error:", e)

        if not text:
            # safety fallbackï¼šçµ¦ä¸€é¡Œ generic technical
            text = (
                "Letâ€™s talk about a technical challenge you recently solved. "
                "Could you walk me through the problem, your approach, and the impact?"
            )
        question_id = f"tech_{spec['index']}_{uuid.uuid4().hex[:8]}"
        tag = "Technical Â· JD-based"
        entry_key = None
        hints = _build_hints_for_generic(hint_level)

    elif q_type == "case":
        # case reasoning é¡Œï¼šJD-based LLM å‡ºé¡Œ
        jd_text = _load_profile_jd_for_questions(profile_id)
        jd_for_llm = _attach_persona_to_jd(jd_text)
        print("[mock case] profile_id=", profile_id, "JD length=", len(jd_text))

        if jd_for_llm.strip():
            try:
                question_text = call_llm_for_question(
                    jd_text=jd_for_llm,
                    mode="case",
                    avoid=used_texts,
                )
                text = (question_text or "").strip()
            except Exception as e:
                print("[mock case] LLM error:", e)

        if not text:
            # safety fallbackï¼šgeneric case prompt
            text = (
                "Imagine you joined our team as a data scientist. "
                "How would you design an end-to-end approach to identify and prioritize opportunities "
                "to improve a key business metric? Talk through assumptions, data, modeling, and how "
                "youâ€™d measure success."
            )

        question_id = f"case_{spec['index']}_{uuid.uuid4().hex[:8]}"
        tag = "Case Â· Reasoning"
        entry_key = None
        hints = _build_hints_for_case(hint_level)

    elif q_type == "project":
        # project deep diveï¼Œæ ¹æ“š primary_project_entry_key + JD åš LLM å•é¡Œ
        jd_text = _load_profile_jd_for_questions(profile_id)
        jd_for_llm = _attach_persona_to_jd(jd_text)
        entry_key = session.get("primary_project_entry_key")
        resume_id = session.get("resume_id")

        if jd_for_llm.strip() and entry_key and resume_id:
            try:
                # ç”¨ entry_key ç•¶ queryï¼Œè®“ RAG æ‰¾å‡ºå°æ‡‰ bullets
                bullets = retrieve_bullets_for_profile(profile_id, entry_key, top_k=8)
            except Exception as e:
                print("[mock project] retrieve error:", e)
                bullets = []

            # entry title å¾ entry_key æ‹†
            if "||" in entry_key:
                entry_title = entry_key.split("||", 1)[1]
            else:
                entry_title = entry_key

            try:
                question_text = call_llm_for_project_question(
                    jd_text=jd_for_llm,
                    entry_title=entry_title,
                    bullets=bullets,
                    previous_qas=None,  # mock æ¨¡å¼å…ˆä¸å¸¶ history
                )
                text = (question_text or "").strip()
            except Exception as e:
                print("[mock project] LLM error:", e)

        if not text:
            # fallbackï¼šç”¨ auto é¡Œåº«çš„ä¸€é¡Œ project é¡
            base = _pick_from_bank(AUTO_BANK, used_ids, used_slugs, category="project")
            question_id = base["id"]
            text = base["text"]
            tag = base["tag"]
            entry_key = entry_key
        else:
            question_id = f"proj_{spec['index']}_{uuid.uuid4().hex[:8]}"
            tag = "Project Â· Deep dive (JD-based)"
        hints = _build_hints_for_generic(hint_level)

    elif q_type == "followup":
        # å„ªå…ˆåƒ spec è£¡ LLM ç”¢å¥½çš„è¿½å•
        custom_text = spec.get("followup_question_text")
        custom_tag = spec.get("followup_tag")

        if custom_text:
            question_id = f"fu_{spec['index']}_{uuid.uuid4().hex[:8]}"
            text = custom_text.strip()
            tag = custom_tag or "Follow-up"
            entry_key = None
            # follow-up é€šå¸¸ä¸éœ€è¦å¤ªå¤šæç¤ºï¼Œé€™è£¡ä»ç”¨ generic çµ±ä¸€æ ¼å¼
            hints = _build_hints_for_generic(hint_level)
        else:
            # fallbackï¼šèˆŠçš„éš¨æ©Ÿè¿½å•é¡Œåº«
            base = _pick_from_bank(FOLLOWUP_BANK, used_ids, used_slugs, category=None)
            question_id = base["id"]
            parent_idx = spec.get("followup_of")
            tag = "Follow-up"
            if isinstance(parent_idx, int):
                tag = f"Follow-up to Q{parent_idx + 1}"
            text = base["text"]
            entry_key = None
            hints = _build_hints_for_generic(hint_level)

    else:
        # safety net
        question_id = f"fallback_{spec['index']}"
        text = "Tell me about a time you had to solve a difficult problem."
        tag = "Behavioral Â· Problem solving"
        entry_key = None
        hints = _build_hints_for_generic(hint_level)

    # æ›´æ–° session çš„ used_question_ids / slugs / texts
    session.setdefault("used_question_ids", [])
    session.setdefault("used_question_slugs", [])
    session.setdefault("used_question_texts", [])

    if question_id and question_id not in session["used_question_ids"]:
        session["used_question_ids"].append(question_id)

    slug = None
    for bank in (BEHAVIORAL_BANK, AUTO_BANK, FOLLOWUP_BANK):
        for q in bank:
            if q["id"] == question_id:
                slug = q.get("slug")
                break
        if slug is not None:
            break
    if slug and slug not in session["used_question_slugs"]:
        session["used_question_slugs"].append(slug)

    if text and text not in session["used_question_texts"]:
        session["used_question_texts"].append(text)

    save_mock_session(session)

    result = {
        "question_id": question_id,
        "question": text,
        "tag": tag,
        "hints": hints or _build_hints_for_generic(hint_level),
    }
    if entry_key:
        result["entry_key"] = entry_key
    return result


# ============================================================
#  Hints builders
# ============================================================

def _build_hints_for_intro(hint_level: str) -> Dict[str, Any]:
    if hint_level == "minimal":
        return {"show": False}
    bullets = [
        "Name, current program / role, and background (e.g., data science student at Columbia).",
        "1â€“2 key experiences relevant to this role (e.g., RA, internship, main projects).",
        "Wrap up with what you're looking for and why this role / company.",
    ]
    structure = "Think of a 60â€“90 second elevator pitch: past â†’ present â†’ future."
    extra = (
        "Avoid reading your resume line by line; focus on a clear narrative and what makes you a good fit."
    )
    return {
        "show": True,
        "bullets": bullets,
        "structure": structure,
        "extra": extra if hint_level == "full" else "",
    }


def _build_hints_for_generic(hint_level: str) -> Dict[str, Any]:
    if hint_level == "minimal":
        return {"show": False}
    structure = "Use STAR: briefly set the situation, explain your task, list 2â€“3 concrete actions, and end with a measurable result or takeaway."
    extra = (
        "Focus on your decisions and reasoning, not just what the team did. "
        "Tie the outcome back to impact on metrics, users, or stakeholders."
    )
    return {
        "show": True,
        "bullets": [],
        "structure": structure,
        "extra": extra if hint_level == "full" else "",
    }


def _build_hints_for_case(hint_level: str) -> Dict[str, Any]:
    if hint_level == "minimal":
        return {"show": False}

    bullets = [
        "Start by restating the goal in your own words and clarifying what success looks like.",
        "List your key assumptions explicitly (about users, data availability, constraints).",
        "Outline your approach in clear steps instead of jumping into details immediately.",
        "Call out the metrics you would monitor and how you would validate your solution.",
    ]
    structure = (
        "Use a top-down structure: goal â†’ assumptions â†’ high-level plan â†’ metrics and validation â†’ trade-offs / risks."
    )
    extra = (
        "Think aloud as you reason through the case. It's better to show your reasoning process clearly "
        "than to jump to a final answer without explaining how you got there."
    )

    return {
        "show": True,
        "bullets": bullets,
        "structure": structure,
        "extra": extra if hint_level == "full" else "",
    }


# ============================================================
#  çµ¦ API ç”¨ï¼šä¾ index å–å‡ºç•¶é¡Œé¡Œç›®
# ============================================================

def get_question_for_index(
    session_id: str,
    index: int,
    seconds_left: Optional[int] = None,  # æ™‚é–“æ¨¡å¼ï¼šå‰©é¤˜ç§’æ•¸
) -> Dict[str, Any]:
    session = load_mock_session(session_id)
    plan = session["question_plan"]

    if index < 0 or index >= len(plan):
        return {
            "done": True,
            "message": "No more questions in this mock interview.",
        }

    # ---------- è®€ä¸Šä¸€é¡Œçš„ reaction ----------
    prev_reaction = ""
    answers = session.get("answers") or []
    if index > 0:
        for a in answers:
            if a.get("index") == index - 1:
                prev_reaction = (a.get("reaction") or "").strip()
                break

    # å…ˆæ‹¿åŸæœ¬è…³æœ¬è£¡çš„ spec
    spec = dict(plan[index])  # åšä¸€ä»½ copy ä¸ç›´æ¥å‹•åŸ dict
    profile_id = session.get("profile_id")

    # ===== æ™‚é–“æ¨¡å¼å°ˆç”¨é‚è¼¯ï¼šå‰©ä¸‹ < 5 åˆ†é˜ â†’ å¼·åˆ¶ behavioral =====
    if (
        session.get("length_type") == "time"
        and seconds_left is not None
        and seconds_left <= 300  # 5 åˆ†é˜ = 300 ç§’
        and spec.get("type") not in ("intro", "behavioral", "followup")
    ):
        spec["type"] = "behavioral"
        plan[index]["type"] = "behavioral"
        session["question_plan"] = plan
        save_mock_session(session)

    # ===== æ ¹æ“š type å‡ºé¡Œ =====
    if spec["type"] == "intro":
        hints = _build_hints_for_intro(session.get("hint_level", "standard"))
        question_text = INTRO_QUESTION["text"]

        bullets = []
        if profile_id:
            try:
                bullets = retrieve_bullets_for_profile(profile_id, question_text, top_k=5)
            except Exception as e:
                print("[mock] retrieve_bullets_for_profile error:", e)

        return {
            "question_id": INTRO_QUESTION["question_id"],
            "question": question_text,
            "tag": INTRO_QUESTION["tag"],
            "hints": hints,
            "index": index,
            "total": len(plan),
            "bullets": bullets,
            "reaction": prev_reaction,  # ğŸ‘ˆ é€šå¸¸ intro = ç¬¬ 0 é¡Œï¼Œé€™è£¡å¤šåŠæ˜¯ç©ºå­—ä¸²
        }
    else:
        q = _generate_non_intro_question(session, spec)
        q["index"] = index
        q["total"] = len(plan)

        bullets = []
        if profile_id:
            try:
                bullets = retrieve_bullets_for_profile(profile_id, q["question"], top_k=5)
            except Exception as e:
                print("[mock] retrieve_bullets_for_profile error:", e)

        q["bullets"] = bullets
        q["reaction"] = prev_reaction  # ğŸ‘ˆ æŠŠä¸Šä¸€é¡Œ reaction é™„åœ¨é€™é¡Œçš„ payload è£¡
        return q


# ============================================================
#  å®Œæˆ mockï¼šWhisper è½‰éŒ„ + åˆ‡æ®µ + è©•åˆ†
# ============================================================

def _load_profile_jd(profile_id: str) -> str:
    """
    å˜—è©¦è®€å– profile çš„ JD æ–‡å­—ï¼Œçµ¦è©•åˆ†ç”¨ã€‚
    âš ï¸ å¦‚æœä½ çš„å°ˆæ¡ˆè·¯å¾‘æˆ– key åç¨±ä¸åŒï¼Œè«‹æ”¹é€™ä¸€æ®µã€‚
    """
    profile_dir = USER_DATA_DIR / "profiles"
    p = profile_dir / f"{profile_id}.json"
    if not p.exists():
        return ""
    try:
        with p.open("r", encoding="utf-8") as f:
            data = _json.load(f)
    except Exception:
        return ""
    return (
        data.get("jd_text")
        or data.get("jd")
        or data.get("job_description")
        or ""
    )


def finalize_mock_session(session_id: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨å‰é¢ /api/mock_answer å·²ç¶“å­˜å¥½çš„ per-question transcriptï¼Œ
    çµ±ä¸€åšä¸€æ¬¡è©•åˆ† + å ±è¡¨è¼¸å‡ºã€‚
    ç¾åœ¨æœƒåŒæ™‚å˜—è©¦è®€å–è©²é¡Œçš„éŒ„å½±æª”ï¼ŒæŠ½å‡ºéŸ³è¨Šåš audio è©•ä¼°ï¼Œ
    ä¸¦é‡å° video è¨ˆç®—ä¸€äº›ç°¡å–®çš„è¦–è¦ºç‰¹å¾µã€‚
    """
    session = load_mock_session(session_id)
    session["completed"] = True
    session["completed_at"] = datetime.datetime.utcnow().isoformat()
    save_mock_session(session)

    profile_id = session["profile_id"]
    try:
        jd_text = _load_profile_jd(profile_id)
    except Exception as e:
        print("[finalize_mock_session] load JD error:", e)
        jd_text = ""

    answers = session.get("answers") or []

    report_questions: List[Dict[str, Any]] = []
    scores: List[int] = []

    # ç…§ index æ’ä¸€ä¸‹
    answers_sorted = sorted(answers, key=lambda a: a.get("index", 0))

    for a in answers_sorted:
        answer_text = a.get("transcript", "") or ""
        question_text = a.get("question_text", "")
        idx = a.get("index")

        # é è¨­ audio å€å¡Šï¼ˆå°±ç®—å¤±æ•—ä¹Ÿæœ‰æ±è¥¿ï¼‰
        audio_block: Dict[str, Any] = {
            "has_audio": False,
            "features": {},
            "delivery_score": None,
            "delivery_comment": "",
        }

        # é è¨­ video å€å¡Š
        video_block: Dict[str, Any] = {
            "has_video": False,
            "features": {},
        }

        if not answer_text.strip():
            # æ²’æœ‰ transcript çš„æƒ…æ³
            eval_result = {
                "overall_score": None,
                "subscores": None,
                "strengths": "",
                "improvements_overview": "No transcript was captured for this question.",
                "improvement_items": [],
                "sample_answer": "",
                "audio": audio_block,
            }
        else:
            # å˜—è©¦æŠ½å‡ºè©²é¡Œçš„ .wav è·¯å¾‘
            audio_wav_path: Optional[Path] = None
            if idx is not None:
                try:
                    audio_wav_path = extract_wav_from_webm(session_id, idx)
                except Exception as e:
                    # æŠ½éŸ³æª”å¤±æ•—ä¸å½±éŸ¿æ•´é«”æµç¨‹ï¼Œåªæ˜¯æ²’ audio è©•ä¼°
                    print(
                        f"[finalize_mock_session] extract_wav_from_webm error for session={session_id}, idx={idx}:",
                        e,
                    )
                    audio_wav_path = None

            # å˜—è©¦è¨ˆç®— video features
            if idx is not None:
                video_path = MOCK_MEDIA_DIR / f"{session_id}_{idx}.webm"
                if video_path.exists():
                    try:
                        v_feats = extract_video_features(video_path)
                        video_block = {
                            "has_video": True,
                            "features": v_feats,
                        }
                    except Exception as e:
                        print(
                            f"[finalize_mock_session] extract_video_features error for session={session_id}, idx={idx}:",
                            e,
                        )

            try:
                eval_result = evaluate_answer(
                    question=question_text,
                    jd_text=jd_text,
                    bullets=[],  # ä¹‹å¾Œå¦‚æœè¦ä¹Ÿå¯ä»¥åœ¨é€™é‚ŠåŠ  RAG
                    user_answer=answer_text,
                    audio_wav_path=audio_wav_path,
                )
            except Exception as e:
                print("[finalize_mock_session] evaluate_answer error:", e)
                eval_result = {
                    "overall_score": None,
                    "subscores": None,
                    "strengths": "",
                    "improvements_overview": "Automatic evaluation failed for this question.",
                    "improvement_items": [],
                    "sample_answer": "",
                    "audio": audio_block,
                }

        # ---- å–æ•´é«”åˆ†æ•¸ï¼ˆå…¼å®¹èˆŠæ¬„ä½ï¼‰ ----
        q_overall_score = eval_result.get("overall_score")
        if q_overall_score is None and isinstance(eval_result.get("score"), int):
            q_overall_score = eval_result.get("score")

        if isinstance(q_overall_score, int):
            scores.append(q_overall_score)

        subscores = eval_result.get("subscores")
        strengths = eval_result.get("strengths", "") or ""
        improvements_overview = (
            eval_result.get("improvements_overview")
            or eval_result.get("improvements")
            or ""
        )
        improvement_items = eval_result.get("improvement_items") or []
        sample_answer = eval_result.get("sample_answer") or ""

        # audio å€å¡Šï¼ˆevaluate_answer å·²ç¶“æœƒå¸¶ audio å›ä¾†ï¼‰
        audio_block = eval_result.get("audio") or audio_block

        report_questions.append(
            {
                "index": idx,
                "question_id": a.get("question_id"),
                "question_text": question_text,
                "answer_text": answer_text,

                # æ–°ç‰ˆæ¬„ä½
                "overall_score": q_overall_score,
                "subscores": subscores,
                "strengths": strengths,
                "improvements_overview": improvements_overview,
                "improvement_items": improvement_items,
                "sample_answer": sample_answer,
                "audio": audio_block,   # æ¯é¡Œçš„ audio è©•ä¼°
                "video": video_block,   # æ¯é¡Œçš„ video ç‰¹å¾µ

                # èˆŠç‰ˆæ¬„ä½ï¼ˆçµ¦é‚„æ²’æ”¹æ‰çš„ template / å‰ç«¯ç”¨ï¼‰
                "score": q_overall_score,
                "improvements": improvements_overview,
            }
        )

    overall_score = int(round(sum(scores) / len(scores))) if scores else None

    result_obj: Dict[str, Any] = {
        "session_id": session_id,
        "profile_id": profile_id,
        "resume_id": session.get("resume_id"),
        "created_at": session.get("created_at"),
        "completed_at": session.get("completed_at"),
        "length_type": session.get("length_type"),
        "hint_level": session.get("hint_level"),
        "num_questions_planned": session.get("num_questions"),
        "questions": report_questions,
        "overall_score": overall_score,
    }

    out_path = MOCK_RESULTS_DIR / f"{session_id}.json"
    with out_path.open("w", encoding="utf-8") as f:
        _json.dump(result_obj, f, ensure_ascii=False, indent=2)

    return result_obj



# ============================================================
#  History / report ç”¨çš„å°å·¥å…·
# ============================================================

def list_mock_sessions_for_profile(profile_id: str) -> List[Dict[str, Any]]:
    """
    çµ¦ history é é¢ç”¨ï¼Œç°¡å–®åˆ—å‡ºé€™å€‹ profile çš„æ‰€æœ‰ mock summaryã€‚
    ï¼ˆç›´æ¥è®€ results è£¡é¢çš„ report.jsonï¼‰
    """
    sessions: List[Dict[str, Any]] = []
    for p in MOCK_RESULTS_DIR.glob("*.json"):
        with p.open("r", encoding="utf-8") as f:
            data = _json.load(f)
        if data.get("profile_id") == profile_id:
            # ç°¡åŒ– summary çµ¦å‰ç«¯ç”¨
            sessions.append(
                {
                    "session_id": data.get("session_id"),
                    "created_at": data.get("created_at"),
                    "overall_score": data.get("overall_score"),
                    "num_questions": len(data.get("questions", [])),
                    "length_type": data.get("length_type"),
                    "hint_level": data.get("hint_level"),
                }
            )
    sessions.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return sessions


def load_mock_result(session_id: str) -> Dict[str, Any]:
    path = MOCK_RESULTS_DIR / f"{session_id}.json"
    if not path.exists():
        raise FileNotFoundError(f"mock result not found: {session_id}")
    with path.open("r", encoding="utf-8") as f:
        return _json.load(f)


# ============================================================
#  LLM-based follow-up æ’å…¥é‚è¼¯
# ============================================================

def _maybe_add_followup_after_answer(session_id: str, answer: Dict[str, Any]) -> None:
    """
    åœ¨æŸä¸€é¡Œå›ç­”ä¹‹å¾Œï¼Œè®“ LLM åˆ¤æ–·è¦ä¸è¦æ’å…¥ follow-upã€‚
    - åªé‡å°é intro / followup é¡Œ
    - follow-up æœƒæ’åœ¨ç•¶å‰é¡Œçš„ä¸‹ä¸€æ ¼ï¼Œä¸¦é‡æ–° re-index question_plan
    - åŒä¸€å€‹ä¸»å•é¡Œæœ€å¤šæ’å…¥ 3 é¡Œ follow-upï¼ˆä¸èƒ½é€£çºŒå‡ºç¾ 4 é¡Œï¼‰
    - å¦‚æœä½¿ç”¨è€…æ˜ç¢ºè¡¨ç¤ºã€Œä¸çŸ¥é“ / æ²’æƒ³æ³•ã€ï¼Œå°±ä¸è¦è¿½å•
    - used in main.py: @app.post("/api/mock_answer")
    """
    try:
        session = load_mock_session(session_id)
    except FileNotFoundError:
        return

    plan = session.get("question_plan") or []
    if not plan:
        return

    idx = answer.get("index")
    if idx is None or not isinstance(idx, int):
        return
    if idx < 0 or idx >= len(plan):
        return

    spec = plan[idx]
    q_type = spec.get("type")

    # ä¸è¦å° intro è‡ªæˆ‘ä»‹ç´¹ / å·²ç¶“æ˜¯ followup å†è¿½å•
    if q_type in ("intro", "followup"):
        return

    # ---- å…ˆçœ‹ç­”æ¡ˆå…§å®¹ï¼šå¦‚æœæ˜ç¢ºè¡¨ç¤ºä¸çŸ¥é“ï¼Œå°±ä¸è¦è¿½å• ----
    current_a = (answer.get("transcript") or "").lower()
    # å¯ä»¥å†è¦–æƒ…æ³åŠ æ›´å¤šé—œéµå­—
    clueless_phrases = [
        "i have no idea",
        "no idea",
        "i don't know",
        "i dont know",
        "i dunno",
        "i am not sure",
        "i'm not sure",
        "i am unsure",
        "i'm unsure",
        "haven't done this before",
        "have not done this before",
        "no experience with this",
        "i don't have experience",
    ]
    if any(p in current_a for p in clueless_phrases):
        # é€™ç¨®å›ç­”å°±ä¸è¦å†å¾€æ­»è£¡è¿½å•ï¼Œç›´æ¥çµæŸ
        return

    # ---- åŒä¸€å€‹ä¸»å•é¡Œæœ€å¤š 3 é¡Œ follow-up ----
    existing_fus_for_this = [
        s for s in plan
        if s.get("type") == "followup" and s.get("followup_of") == idx
    ]
    if len(existing_fus_for_this) >= 3:
        # å·²ç¶“æœ‰ä¸‰é¡Œè¿½å•äº†ï¼Œå°±ä¸è¦å†æ’ç¬¬å››é¡Œ
        return

    profile_id = session.get("profile_id")
    jd_text = _load_profile_jd_for_questions(profile_id)

    # interviewer persona
    persona_text = _build_interviewer_persona(session)
    persona_block = ""
    if persona_text.strip():
        persona_block = (
            "Here is your interviewer persona. Stay consistent with this:\n"
            f"{persona_text}\n\n"
        )

    # æº–å‚™ historyï¼šæ‹¿ç›®å‰ç‚ºæ­¢ï¼ˆå«ç•¶å‰ï¼‰çš„ QAï¼Œæœ€å¤š 3 é¡Œ
    answers_all = session.get("answers") or []
    history = [
        a for a in answers_all
        if isinstance(a.get("index"), int) and a["index"] <= idx
    ]
    history = sorted(history, key=lambda a: a["index"])
    history = history[-3:]  # åªä¿ç•™æœ€å¾Œ 3 é¡Œ

    history_lines = []
    for h in history:
        qi = h.get("index")
        qtxt = h.get("question_text") or ""
        atxt = h.get("transcript") or ""
        history_lines.append(f"Q{qi}: {qtxt}\nA{qi}: {atxt}\n")

    history_block = "\n".join(history_lines)

    current_q = answer.get("question_text") or ""
    current_a_full = answer.get("transcript") or ""

    system_msg = (
        "You are a structured, realistic interviewer for data/ML roles.\n"
        "You decide whether to ask a short, focused follow-up question after the candidate's answer.\n"
        "You only ask a follow-up when there is a clear gap, ambiguity, or interesting detail to explore.\n"
        "Roughly half of the time, you should decide that no follow-up is needed.\n"
        "If the candidate explicitly says they don't know, have no idea, or lack experience with this topic, "
        "you MUST NOT ask a follow-up.\n"
        "Your follow-up must be consistent with the job, previous questions, and the candidate's answer.\n\n"
        f"{persona_block}"
    )

    user_msg = (
        "Job description:\n"
        f"{jd_text}\n\n"
        "Recent questions and answers (from oldest to newest):\n"
        f"{history_block}\n\n"
        "Current question and answer:\n"
        f"Question: {current_q}\n"
        f"Answer: {current_a_full}\n\n"
        "Your task:\n"
        "- Decide whether to ask ONE follow-up question.\n"
        "- Ask a follow-up only if it helps clarify the candidate's decisions, trade-offs, metrics, or impact.\n"
        "- The follow-up should be 1 sentence, concise, and directly related to what the candidate just said.\n\n"
        "Return STRICTLY a JSON object in this format:\n"
        "{\n"
        '  \"need_followup\": true or false,\n'
        '  \"question\": \"your follow-up question here, if any\",\n'
        '  \"tag\": \"a short label like \'Follow-up Â· Metrics\' (optional)\"\n'
        "}\n\n"
        "If you think no follow-up is needed, return:\n"
        "{ \"need_followup\": false }\n"
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.2,
        )
        raw = (resp.choices[0].message.content or "").strip()
        data = _json.loads(raw)
    except Exception as e:
        print("[mock followup] LLM / JSON error:", e)
        return

    need_fu = bool(data.get("need_followup"))
    if not need_fu:
        return

    fu_q = (data.get("question") or "").strip()
    fu_tag = (data.get("tag") or "").strip() or "Follow-up"
    if not fu_q:
        return

    # çœŸæ­£æ’å…¥ follow-up slotï¼šæ”¾åœ¨ç•¶å‰é¡Œçš„ä¸‹ä¸€æ ¼ï¼Œä¸¦é‡æ’ index
    insert_pos = idx + 1
    fu_spec = {
        "index": insert_pos,
        "type": "followup",
        "followup_of": idx,
        "followup_question_text": fu_q,
        "followup_tag": fu_tag,
    }

    plan.insert(insert_pos, fu_spec)
    # é‡æ–°ç·¨ indexï¼Œç¢ºä¿ 0..len-1 é€£çºŒ
    for i, s in enumerate(plan):
        s["index"] = i

    session["question_plan"] = plan
    save_mock_session(session)

    print(f"[mock followup] inserted follow-up after Q{idx}: {fu_q}")


def generate_interviewer_reaction(question: str, answer: str, session: Optional[Dict[str, Any]] = None) -> str:
    """
    æ ¹æ“šè©²é¡Œçš„å•é¡Œèˆ‡å›ç­”ï¼Œç”¢ç”Ÿä¸€å€‹ã€Œé¢è©¦å®˜çš„ä¸€å¥è©±åæ‡‰ã€ã€‚
    - è‡ªç„¶å£èªï¼Œ1 å¥ã€20 å­—ä»¥å…§
    - ä¸å†å•æ–°å•é¡Œï¼Œåªæ˜¯å›é¥‹ / æ‰¿æ¥
    - å¦‚æœç­”æ¡ˆé¡¯ç¤ºå€™é¸äººä¸çŸ¥é“ï¼Œä¹Ÿè©¦è‘—ç¨å¾®å®‰æ’« & æ¥è©±
    - æœƒä¾ç…§ interviewer persona èª¿æ•´èªæ°£
    """
    text = (answer or "").strip()
    if not text:
        return ""

    persona_text = ""
    if session is not None:
        try:
            persona_text = _build_interviewer_persona(session)
        except Exception:
            persona_text = ""

    persona_block = ""
    if persona_text.strip():
        persona_block = (
            "Here is your interviewer persona. Stay consistent with this tone:\n"
            f"{persona_text}\n\n"
        )

    system_msg = (
        "You are a realistic interviewer for data/ML roles.\n"
        "After hearing the candidate's answer, you respond with ONE short sentence.\n"
        "- Be natural, conversational, and concise (max 20 words).\n"
        "- Do NOT ask a new question here; it's just a quick reaction.\n"
        "- If the candidate clearly doesn't know or says 'I have no idea', "
        "briefly reassure them and imply you'll move on.\n\n"
        f"{persona_block}"
    )

    user_msg = (
        "Here is the interview turn:\n"
        f"Question: {question}\n"
        f"Answer: {answer}\n\n"
        "Write your one-sentence reaction:"
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.6,
        )
        reaction = (resp.choices[0].message.content or "").strip()
    except Exception as e:
        print("[mock reaction] LLM error:", e)
        return ""

    # å®‰å…¨è™•ç†ä¸€ä¸‹å¤ªé•·æˆ–ç©ºå­—ä¸²
    if not reaction:
        return ""
    if len(reaction.split()) > 25:
        # ç²—æš´ï¼šåªç•™å‰ 25 å€‹è©
        reaction = " ".join(reaction.split()[:25])

    return reaction

def extract_wav_from_webm(session_id: str, index: int) -> Path:
    """
    è‡ªå‹•å¾ mock_media/<sessionID>_<index>.webm æŠ½å‡ºéŸ³æª” .wav
    """
    webm_path = MOCK_MEDIA_DIR / f"{session_id}_{index}.webm"

    if not webm_path.exists():
        raise HTTPException(status_code=404, detail=f"Video not found: {webm_path}")

    wav_path = MOCK_MEDIA_DIR / f"{session_id}_{index}.wav"

    cmd = [
        "ffmpeg",
        "-y",
        "-i", str(webm_path),
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", "16000",
        "-ac", "1",
        str(wav_path),
    ]

    subprocess.run(cmd, check=True)

    return wav_path