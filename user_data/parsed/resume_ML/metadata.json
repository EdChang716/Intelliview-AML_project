{
  "EDUCATION": "Columbia University New York, NY\nM.S. in Data Science Dec 2026(Expected)\nRelevant Coursework: Exploratory data analysis, Computer Vision, Operational Logistics, Machine Learning, Poisson Process\nBoston University Boston, MA\nB.S. in Data Science, GPA: 3.8/4.0 Sep 2020 - Dec 2024\nMajor in Data Science, Minor in Business Administration\nCoursework: Bayesian Statistics, Computer Systems, Database Design, Data Structure Algorithms, Deep Learning, Hypothesis\nTesting, Linear Algebra, Machine Learning, Multivariable Calculus",
  "SKILLS": "Programming Languages: Python, SQL, R, Rust\nML Libraries & Frameworks: PyTorch, LLM, OpenAI API, LangChain, Transformers, Scikit-Learn\nPlatforms & Developer Tools: AWS, Docker, Git, Google BigQuery, Tableau, Redis\nQuantifying Query Specificity in RAG-based Search Engine\nThis research is part of an ongoing project I contributed to at Boston University's BITLAB, led by\nProfessor Dokyun Lee. We collaborated with a media company that is testing a new\nRetrieval-Augmented Generation (RAG) system. For their experiment, users were split into\n\"control\" and \"treatment\" groups, where the treatment group's queries were reformulated by\nGenAI.\n1. The Research Question\nMy research sought to answer two primary questions:\n1. Is there a statistically significant difference in specificity between user-submitted queries\n(control) and GenAI-reformulated queries (treatment)?\n2. Can we design a reliable, corpus-independent metric to quantify a query's \"specificity\"\nbefore a search is executed to predict its ambiguity and potential performance?\nThis project tested the hypothesis that a GenAI-based search reformulator (the \"treatment\")\nwould produce queries with demonstrably higher semantic specificity than the original\nuser-submitted queries (the \"control\").\n2. Data\nThe dataset provided was a large-scale, anonymized search log from the media company's\nRAG system experiment, delivered as a single parquet file containing over 8.9 million total query\nevents and 34,209 unique query strings. The file included three features: normalized_query,\ngroup, and time. All query strings were pre-normalized (e.g., all lowercase, punctuation\nremoved, and words underscore-separated). The data was pre-segmented into the two distinct\ngroups from the A/B test: the Control Group (user-submitted) contained 7,848,077 events, and\nthe Treatment Group (GenAI-reformulated) contained 1,064,032 events.\n3. Approach and Methodologies\nPart A: Developing the Specificity Construct\nDeveloping this construct was challenging for two reasons: first, traditional metrics like query\nlength are unreliable proxies for specificity; second, published literature often lacks direct,\npre-retrieval metrics for quantifying specificity. Therefore, I designed a new, two-part composite\nscore system based on an evidence-gathering process from highly-cited academic papers in\ninformation systems and semantic analysis. This score contains two measurements highly\ncorrelated with specificity: Named Entity Count and Entity Granularity.\nThe rationale for Named Entity Count is that entities act as semantic constraints. A query with\nmore entities (e.g., \"Apple Park cupertino\") is inherently more specific than one with fewer (e.g.,\n\"tech companies\"). To measure this, I used an LLM to perform Named Entity Recognition (NER)\non all 34,209 unique queries, extracting and classifying entities (e.g., PERSON, LOCATION,",
  "COURSES": "",
  "OTHER": "(646) 992-7153 | cy2816@columbia.edu | linkedin.com/in/olivery0307 | https://github.com/Olivery0307\nPalAI New York, NY\nAI/ML Intern Sep 2025 – Present\n• Engineered and prototyped a core ML and AI-driven college application and essay feedback platform\n• Designed a data pipeline to standardize and augment 100+ essay/feedback pairs and fine-tuning the model with the Gemini API\n• Collaborated with cross-functional backend and frontend engineers to deploy ML models into a production-level user experience\nBoston University Questrom School of Business Boston, MA\nResearch Assistant in Applied AI in Business Aug 2023 - Jan 2025\n• Collaborated with 5+ academic researchers to design and implement four AI solutions tailored to specific business challenges\n• Analyzed over 50,000 user queries to measure the impact of query characteristics on RAG generation and user behaviors\n• Engineered a pipeline that uses an LLM to label artwork genres from unsupervised clusters, achieving zero-shot topic modeling\nBU Spark! Boston, MA\nData Science Technical Project Manager Sep 2024 - Dec 2024\n• Managed six data analysis and visualization projects, coordinating with clients and facilitating student communication\n• Provided technical assistance and leadership on data visualization projects leveraging Python, Tableau, and Google BigQuery\n• Configured dataset and software setup for Notion and GitHub, ensuring teams had resources for project milestones\nGIGA RESET Shanghai, CN\nData Science Summer Intern Jun 2024 - Aug 2024\n• Designed a data processing pipeline for the RESET Air project with Python, reducing manual workload by 80%\n• Automated creation of scorecards by importing Python libraries, optimizing slide generation and data visualization\nTaiwanese Overseas Students Association at Boston University Boston, MA\nPresident Apr 2023 - May 2024\n• Organized events and promote Taiwanese culture for over 300 Taiwanese students and general members in Boston University\n• Coordinated and distributed work to 20 executive board members, ensuring efficient and event planning and execution\nEntity Granularity stands for the intrinsic specificity of an individual entity. The reason this is\nneeded as a supplement to entity count is that not all entities are equal; a query with two\nlow-granularity entities (e.g., \"tech companies california\") is far less specific than a query with\ntwo high-granularity entities (e.g., \"Apple Park cupertino\"). To measure this, I queried large-scale\nKnowledge Graphs (KGs) using a multi-KG approach to select the best source for each entity\ntype. For example, for LOCATIONs, I used the GeoNames API, scoring specificity based on the\nentity's featureCode (e.g., a city is more granular than a country).For MISC (abstract\nconcepts), I used WordNet (via NLTK) to measure conceptual depth. Finally, a query's\ngranularity score was aggregated by computing the mean granularity of all its entities.\nPart B: Statistical Analysis (Hypothesis Testing)\nMy hypothesis was that the Treatment group's queries would have significantly higher scores for\nboth entity count and granularity score.\n● Assumption Checking: The data was count-based, non-normally distributed, and had\nunequal variances. This ruled out a parametric t-test.\n● Primary Statistical Test: The non-parametric Mann-Whitney U test was selected as the\nappropriate method to compare the distributions of the two independent (Control vs.\nTreatment) groups.\n● Analysis: I ran two separate Mann-Whitney U tests: one comparing the entity count\ndistributions and one comparing the granularity score distributions.\n4. My Findings\nThe analysis confirmed the hypothesis with high statistical significance. For one, the GenAI\nreformulation significantly increased the number of entities. The Treatment group had a mean of\n0.93, compared to the Control group's 0.79. This difference was highly statistically significant.\nFor another, the tool significantly increased the semantic granularity. The Treatment group had a\nhigher mean granularity score (0.04) than the Control group (0.03). This was also highly\nstatistically significant. In conclusion, the GenAI-reformulated queries were demonstrably more\nspecific, and more semantically constrained than the original user queries. The tool\nsuccessfully transformed vague queries into more actionable, entity-grounded inputs.\n5. Computational Environment\n● Programming Language: Python; Report: PowerPoint, Latex(Overleaf)\n● Main Libraries:\n○ Data Analysis & Manipulation: pandas, numpy\n○ Statistical Analysis: scipy.stats for mannwhitneyu, levene) statsmodels\n○ Data Visualization: matplotlib, seaborn\n○ API/KG Access: requests (for GeoNames/Wikidata APIs),\ngoogle-generative-ai (for NER), nltk (for WordNet)"
}