---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:4965
- loss:LoggingMNRLoss
base_model: sentence-transformers/all-mpnet-base-v2
widget:
- source_sentence: 'JOB DESCRIPTION:

    Company DescriptionAs an Etsy employee, you can do the work you love, be yourself,
    and make an impact in the lives of millions. Our commitments to diversity and
    inclusion, team culture and the spaces where we work all reflect our mission to
    keep commerce human.Job DescriptionData scientists at Etsy use rigorous methods
    to generate insights that inform product, engineering, and business decisions
    across the company. We collaborate with partner teams through all stages of development:
    actively uncovering opportunity areas, crafting experiments to test hypotheses,
    analyzing the impact of our efforts, and highlighting takeaways.Learning new skills
    and techniques is not only a requirement but a perk of the job! We are always
    looking for opportunities to grow. Our mission is to guide our partner teams with
    data and insights and tell the story of how we attract and retain our users -
    to teams, to senior management, and to the community.We''re hiring partners for
    a few key Product teams, including the search experience and fulfillment teams.
    These groups are focused are enhancing their areas of the core shopping experience
    to connect more buyers with sellers offering just the items they need.This role
    is located in Brooklyn, NY and is part of our Analytics and Strategic Finance
    team.About the Role* Transform raw data into impactful analysis characterized
    by strong data governance/documentation, rigorous techniques, and actionable recommendations.*
    Partner with product, engineering, design teams; use behavioral and transactional
    data to shed light on our users and our product experience, identify areas for
    improvement, and drive decision-making.* Identify actionable metrics to understand
    the performance of products, develop reporting dashboards to track progress, and
    ingrain these metrics into teams'' day-to-day decision-making.* Lead experimentation,
    help teams set strong hypotheses, and deliver robust analysis of experiment results.*
    Continually evaluate and refine your technical toolkit; teach what you learn to
    the team.QualificationsAbout You* 2-3 years experience as an analyst or in a quantitative
    role in which you extracted meaning from big data sets with little engineering
    support.* Understand the principles of A/B testing, and love crafting and analyzing
    experiments.* Proficient in SQL and have familiarity with either R or Python.
    Bonus points for experience with the Hadoop ecosystem or additional scripting
    languages (Scala, PHP, Ruby, etc.).* Experience with Looker, Tableau, or other
    data visualization software a plus.* You look for ways to advance your technical
    skill set and to apply new methods to impactful questions.* You can communicate
    your insights verbally, visually, and in writing and care deeply about the quality
    and integrity of your work.* You want to play a key role on a cross-functional
    team.* You are passionate about subjects such as experimental design, data visualization,
    and statistical analysis techniques.Additional InformationAt Etsy, we believe
    that a diverse, equitable and inclusive workplace makes us a more relevant, more
    competitive, and more resilient company. We welcome people from all backgrounds,
    ethnicities, cultures, and experiences. Etsy is an equal opportunity employer.
    We do not discriminate on the basis of race, color, ancestry, religion, national
    origin, sexual orientation, age, citizenship, marital or family status, disability,
    gender identity or expression, veteran status, or any other legally protected
    status. We will ensure that individuals with disabilities are provided a reasonable
    accommodation to participate in the job application or interview process, to perform
    essential job functions, and to receive other benefits and privileges of employment.
    While Etsy supports visa sponsorship, sponsorship opportunities may be limited
    to certain roles and skillsets.


    INTERVIEW QUESTION:

    Can you walk me through one of the A/B experiments you designed for improving
    user engagementâ€”how you decided on the metrics, set up the test, and interpreted
    the results?'
  sentences:
  - Designed and analyzed over 20 A/B experiments leveraging Python and statistical
    methods, driving a 15% improvement in user engagement on search and fulfillment
    features.
  - Collaborated with cross-functional teams to establish data governance and system
    controls, ensuring compliance with architectural standards and protecting data
    confidentiality, integrity, and availability for critical financial and operational
    reporting.
  - Executed routine instrument maintenance and troubleshooting using laboratory information
    systems (LIS), reducing equipment downtime by 15% and enhancing workflow efficiency
    during evening shifts.
- source_sentence: 'JOB DESCRIPTION:

    At Housecall Pro, we''re focused on making the everyday lives of home services
    professionals better by helping them to incorporate mobile software into their
    workflow...freeing them to focus on what they do best. The home services industry
    remains vast ($700b+) but untouched by technology and unencumbered by a dominant
    competitor. The 3M+ mobile businesses in home services are facing dramatic change
    as the world shifts from offline to online. Along this journey, we are quickly
    becoming the backbone operating system for service labor across the US and Canada.
    We have rocketed from MVP to the #1 ranked SaaS software platform in just over
    2 years...which has made us the fastest-growing tech startup in San Diego!


    Our vision is to revolutionize the home services experience.


    Our mission is to unlock the potential of every Pro.


    The role:

    As a Lead Data Architect/Engineer, you will lead a technical team that architects,
    builds, maintains, scales, monitors, administrates and secures Housecall Pro''s
    Analytics infrastructure. You will actively work in a multi-disciplinary fast-paced
    environment, drive innovation and satisfy the analytics needs of your stakeholder.
    Your ultimate goal is to enable Housecall Pro and it''s customers to be data-driven
    and go from good to great; and you will do so by creating a scalable data system
    that enables us to deliver best-in-class analytics products to HCP and HCP Pros
    in the face of massive growth.


    This role requires a broad range of skills and abilities; you will be the functional
    lead, drive innovation, manage staff and do the work. Your primary responsibility
    is to enable data access, build data governance, data infrastructure and data
    products by architecting, maintaining, scaling, monitoring & securing Housecall
    Pro''s

    We ML production system (AWS, Airflow, Python, Github, Gitlab)

    Data Warehouse (Snowflake)

    ETL system & data pipelines (Segment, Stitchdata, Airflow, Python)

    BI system (Tableau Online, Amplitude)

    (Current analytics stack in parentheses)


    Skills:

    You have planned, built & managed data infrastructures in a public cloud.

    You have 6+ years of experience as a Data Engineer/Data Architect and have architected
    distributed data platforms.

    You can estimate, design, build and own a business initiative end to end.

    You have in-depth experience with MySQL databases and Snowflake''s data warehouse

    You have managed a business intelligence system

    You are proficient in at least one programming languages like Python, Scala and
    Java

    You have familiarity with big data technologies like Hadoop, Spark, Hive

    You are comfortable with setting and meeting SLAs for data availability and quality

    You have an understanding of Machine Learning / AI principles in data engineering

    You have consistently engaged in mentoring, training and reviewing other data
    engineers on the team.

    You''ve worked in an Agile environment. You thrive on scrum. You make opportunities
    to bring value sooner rather than later.

    You value data-driven decisions. You are always looking for opportunities to quickly
    produce the right data to make decisions quickly. You keep cool under pressure.

    You are a self-driven, highly motivated technologist who works with a high degree
    of autonomy, is able to prioritize effectively and shape a company''s data engineering
    roadmap.

    Our Analytics team is extraordinary. We are empathetic, hard working and focused
    on easy data and information access. Housecall Pro is a data-driven company, and
    we are the "insights engine" of the organization We help HCP and our HCP service
    professionals optimize and grow their businesses. Join us!


    INTERVIEW QUESTION:

    Can you walk me through the process you followed to design and implement those
    ETL pipelines, and what were some of the key challenges you faced in ensuring
    99.9% SLA compliance?'
  sentences:
  - Developed and revised SOPs, test methods, and OOS investigation protocols, improving
    laboratory documentation accuracy by 15% under supervisory guidance.
  - Developed and automated low-noise measurement protocols using Python and MATLAB,
    improving data acquisition efficiency by 25% in cryogenic superconducting circuit
    experiments.
  - Led the design and implementation of end-to-end ETL pipelines with Python, Airflow,
    and Stitchdata, increasing data availability and reliability to meet 99.9% SLA
    compliance across multiple data sources.
- source_sentence: 'JOB DESCRIPTION:

    Scientist - Mathematics

    Do you want to tackle hard science problems that will make a difference in the
    world?

    Are you tired of academic politics and the publication treadmill?

    Instead of chasing big data, writing software to serve internet ads, or training
    quant models, wouldnâ€™t you like to do something more meaningful?

    We are looking for mathematicians with good problem-solving skills who are willing
    to tackle challenging research in various areas of integro-differential equations,
    numerical convergence, algebraic topology, and optimization. We are open to a
    diverse set of research backgrounds whether in pure or applied mathematics.


    At Verseon, we are developing a process for systematically designing novel drug
    molecules that bind to important disease-causing protein targets. We are not looking
    for canned solutions and quick fixes. You will have the opportunity to pursue
    hard research problems and develop novel and efficient mathematical methods.


    Qualifications:

    PhD in mathematics, applied mathematics, mathematical physics, computer science,
    or related fields

    Research background in analysis, dynamics, geometry, PDEs, or topology would be
    a plus

    Coding proficiency preferred

    Location: Fremont, CA


    INTERVIEW QUESTION:

    Can you walk me through the process you followed to develop these numerical methods
    and how you identified opportunities to improve the convergence rates?'
  sentences:
  - Developed novel numerical methods for solving integro-differential equations,
    improving computational convergence rates by 25% using Python and MATLAB.
  - Developed and deployed predictive models using R and Python to analyze clinical
    trial data, improving patient outcome predictions by 15%.
  - Designed and optimized chimeric antigen receptor (CAR) motifs for iPSC-derived
    NK cells, enhancing targeted antitumor activity and increasing CAR expression
    efficiency by 25%.
- source_sentence: 'JOB DESCRIPTION:

    Hello,


    Hope you are doing great!


    If you are comfortable with the requirement, kindly respond with below information
    and your updated resume( word format ) ASAP


    Full Name:


    Current Location:


    Relocation:


    Contact No Primary( Not Google Number ):


    Email:


    Skype ID:


    Work Authorization:


    Availability( Any notice period required to join in project ):


    Currently working(Yes/No):


    LinkedIn( if you have):


    Hourly Pay Rate: $53/hr on W2( without benefits ) or $60/hr on 1099


    Please check the below requirement


    Position: Data Engineer â€“ 100% Remote


    Duration: 12+ Months Contract( Long term )


    Total Positions: 2


    Need Healthcare claims exp


    185747--Data Engineer- 100% remote


    PRIMARY RESPONSIBILITIES

    Build and maintain large-scale batch and real-time ETL pipelines in a Google Cloud
    Platform architecture (BigQuery, Dataproc, Firestore, etc.).

    Design and support Data Lakes and Marts in Google CloudStorage, BigQuery, and
    NoSQL (Google Firestore & MongoDB).

    Implement high-quality test-driven code, ?participate in code reviews, and own
    the development of medium-size features.

    Support business operational activities including generating marketing leads and
    helping ensure we are meeting our clientâ€™s expectations.

    Collaborate with data scientists and business analysts to enable self-service
    analytics and reporting.

    Support Production systems off hours.

    POSITION REQUIREMENTS


    Experience:

    2+ years of industry experience in software development or data engineering.

    2+ years extracting insights from data.

    1+ years hands-on experience manipulating data and building data pipelines.

    Hands-on experience with relational and NOSQL databases.

    Strong appreciation for test-driven development and developing code standards.

    Competencies:

    Strong understanding of Python or Scala. We use Python, Pyspark, and SQL.

    Understanding of workflow orchestrators (ex. Airflow, Oozie, or Azkaban).

    Understanding of test-driven development techniques.

    Ability to proactively communicate and collaborate across a growing distributed
    team.

    Quality Control:

    Help define and manage overall data quality objectives.

    Implement high-quality source code that passes tests.

    Achieve and maintain team defined quality thresholds that mitigate solution risks.

    Thanks and Regards


    Prasad Mamidela | Kairos Technologies Inc.


    Direct Number: 972.777.9484 | M: 315.969.9307


    433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA


    http://www.kairostech.com


    LinkedIn: https://www.linkedin.com/in/prasadmamidela


    Job Type: Contract


    Pay: $130,044.00 - $184,087.00 per year


    Schedule:

    Monday to Friday

    Experience:

    Total IT: 8 years (Required)

    NoSQL: 1 year (Required)

    Data Engineering: 3 years (Required)

    Manipulating data and building data pipelines: 2 years (Required)

    Python: 1 year (Preferred)

    Contract Renewal:

    Possible

    Full Time Opportunity:

    Yes

    Work Location:

    Fully Remote

    Work Remotely:

    Yes


    INTERVIEW QUESTION:

    Can you walk me through how you introduced test-driven development in Airflow
    workflows and what specific challenges you faced during that process?'
  sentences:
  - Developed and delivered comprehensive training programs on molecular genetics
    techniques and Single Bead Antibody Screen analysis, improving staff proficiency
    and reducing technical errors by 25%.
  - Led monthly financial performance reviews and created predictive models to optimize
    budgeting processes, enabling timely identification of variances and supporting
    a 10% reduction in operational costs.
  - Implemented test-driven development practices and code quality standards within
    Airflow orchestrated workflows, increasing pipeline reliability and reducing production
    incidents by 40%.
- source_sentence: 'JOB DESCRIPTION:

    SECURITY INCIDENT RESPONSE TEAM (SIRT) supports and enables a comprehensive technical
    Cyber Defense program for the firm while increasing awareness of current and potential
    Cyber Threats. Works across the organization to operate efficiently, provide technical
    investigative support and mitigate threats to the firm.


    Business Unit Overview

    Led by the Chief Information Security Officer (CISO), Technology Risk secures
    Goldman Sachs against hackers and other cyber threats. We are responsible for
    detecting and preventing attempted cyber intrusions against the firm, helping
    the firm develop more secure applications and infrastructure, developing software
    in support of our efforts, measuring cybersecurity risk, and designing and driving
    implementation of cybersecurity controls. The team has global presence across
    the Americas, APAC, India and EMEA. Within Technology Risk, The Security Incident
    Response Team (SIRT) identifies malicious activity, manages the lifecycle of vulnerabilities
    within GS technologies, and investigates and manages threats across the firm.
    We are a team of security, software, and product engineers that allow the firm
    to respond appropriately to firm risks through the use of detection model, security
    architecture, and cutting-edge cyber threat analysis to manage internal and external
    threats against the firm.

    Role

    In this role, you will be responsible for the creation of data science solutions
    to extract key signals from cyber security data sources. You will work with data
    science and machine learning technologies to design, test and deliver world class
    models that decrease cyber security risk. The ideal candidate should have experience
    in applying statistics, building and testing models and working with big data
    solutions in a distributed computing environment. Experience with programming
    languages including Java, Python, R,& SQL. Candidates will be able to convey complicated
    technical analysis to senior management via investigation synopses, graphical
    depictions of attacks, and comprehensive presentations.


    Job Responsibilities

    â€¢ Responsible for the creation of innovative methodologies for extracting key
    parameters from big data originating from various sensors.

    â€¢ Utilize expertise in machine learning, statistical data analytics, and predictive
    analytics to help implement analytics tied to cyber security and hunting methodologies
    and applications

    â€¢ Design, develop, test and deliver complex analytics in a range of programming
    environments on large data sets

    â€¢ Apply latest technologies in machine learning, data mining, and predictive analytics
    to correlate the big datasets and events, and derive dynamic cybersecurity rules.

    â€¢ Generate highly accurate and near real-time security alerts based on the dynamic
    rules. Collaborate with a global team to continually operate and improve a world-class
    cyber program by driving the uplift of sensory tools, detection tuning, and access
    to data sources to increase detection effectiveness by applying data analytics.


    Basic Qualifications

    â€¢ Strong English verbal and written communication skills.

    â€¢ Ability to multi-task and prioritize work effectively.

    â€¢ Highly motivated self-starter who can provide thought leadership in big data
    analytics.

    â€¢ Responsive to challenging tasks

    â€¢ Ability to document and explain technical details in a concise and understandable
    manner.

    â€¢ Strong sense of ownership and driven to manage tasks to completion.

    â€¢ Minimum 3 years full time work and post graduate work experience

    â€¢ 3 - 5 yearsâ€™ experience with scalable distributed data processing, management,
    and visualization tools (e.g. Hadoop, Apache Spark, etc.).

    â€¢ Proven knowledge of industry leading scripting tools such as Python, Powershell,
    R and SQL


    Preferred Qualifications

    â€¢ Bachelor of Science in Computer Science, System/Computer Engineering, Data Science
    and Machine Learning, Cyber-Security, or Information Security is preferred.

    â€¢ Four (4) years of additional work experience may be substituted in lieu of a
    Bachelorâ€™s Degree. Bachelors of Science/Arts in Forensic Computing, System/Computer
    Engineering, Data Science, Engineering, Operations Research, or Decision Science
    will be considered.

    â€¢ Strong mathematical background (linear algebra, probability and statistics).

    â€¢ Previous work experience in Cyber Security field is a plus.

    â€¢ Excellent oral, written, and presentation communication skills required.

    #techriskcybersecurity


    The Goldman Sachs Group, Inc. is a leading global investment banking, securities
    and investment management firm that provides a wide range of financial services
    to a substantial and diversified client base that includes corporations, financial
    institutions, governments and individuals. Founded in 1869, the firm is headquartered
    in New York and maintains offices in all major financial centers around the world.


    Ã‚Â© The Goldman Sachs Group, Inc., 2020. All rights reserved Goldman Sachs is an
    equal employment/affirmative action employer Female/Minority/Disability/Vet.


    INTERVIEW QUESTION:

    Can you walk me through the process you followed to design and implement those
    real-time security alerting rules, and how you decided which statistical methods
    and predictive models to use?'
  sentences:
  - Analyzed complex datasets using Excel and SPSS to identify key trends, enabling
    management to prioritize business strategies and increase reporting accuracy by
    15%.
  - Executed complex SQL queries and optimized database workflows to extract and prepare
    large-scale telecom datasets, supporting advanced statistical analyses that informed
    network optimization strategies.
  - Designed and implemented real-time security alerting rules leveraging statistical
    analytics and predictive modeling, reducing incident response time by 25%.
pipeline_tag: sentence-similarity
library_name: sentence-transformers
metrics:
- cos_sim_accuracy@1
- cos_sim_accuracy@3
- cos_sim_accuracy@5
- cos_sim_accuracy@10
- cos_sim_precision@1
- cos_sim_precision@3
- cos_sim_precision@5
- cos_sim_precision@10
- cos_sim_recall@1
- cos_sim_recall@3
- cos_sim_recall@5
- cos_sim_recall@10
- cos_sim_ndcg@10
- cos_sim_mrr@10
- cos_sim_map@100
model-index:
- name: SentenceTransformer based on sentence-transformers/all-mpnet-base-v2
  results:
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: ft mpnet v2
      type: ft_mpnet_v2
    metrics:
    - type: cos_sim_accuracy@1
      value: 0.4975845410628019
      name: Cos Sim Accuracy@1
    - type: cos_sim_accuracy@3
      value: 0.6996779388083736
      name: Cos Sim Accuracy@3
    - type: cos_sim_accuracy@5
      value: 0.7528180354267311
      name: Cos Sim Accuracy@5
    - type: cos_sim_accuracy@10
      value: 0.8325281803542673
      name: Cos Sim Accuracy@10
    - type: cos_sim_precision@1
      value: 0.4975845410628019
      name: Cos Sim Precision@1
    - type: cos_sim_precision@3
      value: 0.23322597960279118
      name: Cos Sim Precision@3
    - type: cos_sim_precision@5
      value: 0.15056360708534622
      name: Cos Sim Precision@5
    - type: cos_sim_precision@10
      value: 0.08325281803542672
      name: Cos Sim Precision@10
    - type: cos_sim_recall@1
      value: 0.4975845410628019
      name: Cos Sim Recall@1
    - type: cos_sim_recall@3
      value: 0.6996779388083736
      name: Cos Sim Recall@3
    - type: cos_sim_recall@5
      value: 0.7528180354267311
      name: Cos Sim Recall@5
    - type: cos_sim_recall@10
      value: 0.8325281803542673
      name: Cos Sim Recall@10
    - type: cos_sim_ndcg@10
      value: 0.6638613318194824
      name: Cos Sim Ndcg@10
    - type: cos_sim_mrr@10
      value: 0.6101037113718277
      name: Cos Sim Mrr@10
    - type: cos_sim_map@100
      value: 0.6159196700425482
      name: Cos Sim Map@100
---

# SentenceTransformer based on sentence-transformers/all-mpnet-base-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) <!-- at revision e8c3b32edf5434bc2275fc9bab85f82640a19130 -->
- **Maximum Sequence Length:** 384 tokens
- **Output Dimensionality:** 768 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/huggingface/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False, 'architecture': 'MPNetModel'})
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ðŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'JOB DESCRIPTION:\nSECURITY INCIDENT RESPONSE TEAM (SIRT) supports and enables a comprehensive technical Cyber Defense program for the firm while increasing awareness of current and potential Cyber Threats. Works across the organization to operate efficiently, provide technical investigative support and mitigate threats to the firm.\n\nBusiness Unit Overview\nLed by the Chief Information Security Officer (CISO), Technology Risk secures Goldman Sachs against hackers and other cyber threats. We are responsible for detecting and preventing attempted cyber intrusions against the firm, helping the firm develop more secure applications and infrastructure, developing software in support of our efforts, measuring cybersecurity risk, and designing and driving implementation of cybersecurity controls. The team has global presence across the Americas, APAC, India and EMEA. Within Technology Risk, The Security Incident Response Team (SIRT) identifies malicious activity, manages the lifecycle of vulnerabilities within GS technologies, and investigates and manages threats across the firm. We are a team of security, software, and product engineers that allow the firm to respond appropriately to firm risks through the use of detection model, security architecture, and cutting-edge cyber threat analysis to manage internal and external threats against the firm.\nRole\nIn this role, you will be responsible for the creation of data science solutions to extract key signals from cyber security data sources. You will work with data science and machine learning technologies to design, test and deliver world class models that decrease cyber security risk. The ideal candidate should have experience in applying statistics, building and testing models and working with big data solutions in a distributed computing environment. Experience with programming languages including Java, Python, R,& SQL. Candidates will be able to convey complicated technical analysis to senior management via investigation synopses, graphical depictions of attacks, and comprehensive presentations.\n\nJob Responsibilities\nâ€¢ Responsible for the creation of innovative methodologies for extracting key parameters from big data originating from various sensors.\nâ€¢ Utilize expertise in machine learning, statistical data analytics, and predictive analytics to help implement analytics tied to cyber security and hunting methodologies and applications\nâ€¢ Design, develop, test and deliver complex analytics in a range of programming environments on large data sets\nâ€¢ Apply latest technologies in machine learning, data mining, and predictive analytics to correlate the big datasets and events, and derive dynamic cybersecurity rules.\nâ€¢ Generate highly accurate and near real-time security alerts based on the dynamic rules. Collaborate with a global team to continually operate and improve a world-class cyber program by driving the uplift of sensory tools, detection tuning, and access to data sources to increase detection effectiveness by applying data analytics.\n\nBasic Qualifications\nâ€¢ Strong English verbal and written communication skills.\nâ€¢ Ability to multi-task and prioritize work effectively.\nâ€¢ Highly motivated self-starter who can provide thought leadership in big data analytics.\nâ€¢ Responsive to challenging tasks\nâ€¢ Ability to document and explain technical details in a concise and understandable manner.\nâ€¢ Strong sense of ownership and driven to manage tasks to completion.\nâ€¢ Minimum 3 years full time work and post graduate work experience\nâ€¢ 3 - 5 yearsâ€™ experience with scalable distributed data processing, management, and visualization tools (e.g. Hadoop, Apache Spark, etc.).\nâ€¢ Proven knowledge of industry leading scripting tools such as Python, Powershell, R and SQL\n\nPreferred Qualifications\nâ€¢ Bachelor of Science in Computer Science, System/Computer Engineering, Data Science and Machine Learning, Cyber-Security, or Information Security is preferred.\nâ€¢ Four (4) years of additional work experience may be substituted in lieu of a Bachelorâ€™s Degree. Bachelors of Science/Arts in Forensic Computing, System/Computer Engineering, Data Science, Engineering, Operations Research, or Decision Science will be considered.\nâ€¢ Strong mathematical background (linear algebra, probability and statistics).\nâ€¢ Previous work experience in Cyber Security field is a plus.\nâ€¢ Excellent oral, written, and presentation communication skills required.\n#techriskcybersecurity\n\nThe Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.\n\nÃ‚Â© The Goldman Sachs Group, Inc., 2020. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.\n\nINTERVIEW QUESTION:\nCan you walk me through the process you followed to design and implement those real-time security alerting rules, and how you decided which statistical methods and predictive models to use?',
    'Designed and implemented real-time security alerting rules leveraging statistical analytics and predictive modeling, reducing incident response time by 25%.',
    'Analyzed complex datasets using Excel and SPSS to identify key trends, enabling management to prioritize business strategies and increase reporting accuracy by 15%.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[ 1.0000,  0.6283, -0.0382],
#         [ 0.6283,  1.0000, -0.0151],
#         [-0.0382, -0.0151,  1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

## Evaluation

### Metrics

#### Information Retrieval

* Dataset: `ft_mpnet_v2`
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator)

| Metric               | Value      |
|:---------------------|:-----------|
| cos_sim_accuracy@1   | 0.4976     |
| cos_sim_accuracy@3   | 0.6997     |
| cos_sim_accuracy@5   | 0.7528     |
| cos_sim_accuracy@10  | 0.8325     |
| cos_sim_precision@1  | 0.4976     |
| cos_sim_precision@3  | 0.2332     |
| cos_sim_precision@5  | 0.1506     |
| cos_sim_precision@10 | 0.0833     |
| cos_sim_recall@1     | 0.4976     |
| cos_sim_recall@3     | 0.6997     |
| cos_sim_recall@5     | 0.7528     |
| cos_sim_recall@10    | 0.8325     |
| **cos_sim_ndcg@10**  | **0.6639** |
| cos_sim_mrr@10       | 0.6101     |
| cos_sim_map@100      | 0.6159     |

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 4,965 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                           | sentence_1                                                                         |
  |:--------|:-------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|
  | type    | string                                                                               | string                                                                             |
  | details | <ul><li>min: 62 tokens</li><li>mean: 361.48 tokens</li><li>max: 384 tokens</li></ul> | <ul><li>min: 24 tokens</li><li>mean: 36.36 tokens</li><li>max: 57 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | sentence_1                                                                                                                                                                                                                              |
  |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>JOB DESCRIPTION:<br>Node JS Location: Chennai (Greams Road) Skills: Good communications skills Experience: 1+ - 4 years Education Qualification: BE (CSE), BCA, BSC (CS) Must have experience in Node.JS, MongoDB, Express.js, Passport.js, Mongoose Job description for Nodejs:<br><br>Extensive hands-on experience in Node.JS, MongoDB and Angular.JS<br>Strong coding and designing skills in JavaScript, HTML, and CSS in addition to AngularJS<br>Should have knowledge on RESTful JSON APIs.<br>Experience with version control tool like SVN and Git<br>Should have asynchronous coding techniques (callbacks / Promises / the async library.)<br>Should have knowledge on linux commands Collaborate with the team to create a fast and scalable system You must be the excellent problem solver with a passion to self-learn and implement web technologies (frontend + backend).<br><br>Register with us today<br>Submit your CV to hr@casperon.com<br><br>INTERVIEW QUESTION:<br>Can you walk me through a specific example of how you implemented Promises and async/...</code> | <code>Implemented asynchronous coding patterns with Promises and async/await in JavaScript, reducing server response time by 30% and enhancing user experience.</code>                                                                  |
  | <code>JOB DESCRIPTION:<br>About gettacar:<br><br>gettacar is a direct-to-your-door, online platform that enables users to buy, finance, and trade used cars. The old way of buying cars is outdated. We want buying a car to be as simple as buying a pair of shoes and as trustworthy as online banking. By controlling the process from beginning to end, we allow customers to shop a wide inventory of cars, choose their terms, and have the car delivered right to their door. We even give 7 days to decide if you want to keep it. This is the future of car buying.<br><br>What we are looking for:<br><br>gettacar is looking for a Data Engineer to join our growing Analytics team. The Data Engineer position will be responsible for building out and developing our analytics data warehouse.<br><br>Weâ€™ll count on you to:<br>Structure, manage, and ensure accuracy of data in a SQL data warehouse environment to support various levels of analytics products using the latest ETL methods and technologies.<br>Collaborate with analytics and business team...</code>          | <code>Developed and optimized complex data warehouse schemas, views, and stored procedures to support advanced business intelligence reporting, resulting in a 25% increase in data accessibility across cross-functional teams.</code> |
  | <code>JOB DESCRIPTION:<br>Apply Now<br><br><br>JOB SUMMARY:<br><br>The Corporate Senior Business Intelligence (BI) Analyst works with a variety of data analysis tools and information systems to transform data within those systems into visualizations and reporting tools that drive decisions related to organizational expansion, continuous improvement, policies and procedures, budgeting and forecasting, and operational efficiency. This position reports to the Chief Operating Officer (COO).<br><br>JOB DUTIES AND RESPONSIBILITIES:<br>Develop and prepare strategies for BI processes for organization.<br>Perform analysis of organization business processes and provide relevant results.<br>Prepare architecture for data, design strategies, and provide business object solutions as per requirements.<br>Supervise efficient working of all BI projects and analyze needs and perform appropriate tests on the same.<br>Administer projects, prepare updates, and implement all phases for the project to achieve all project objectives to include using Tea...</code>    | <code>Managed and optimized ETL processes and data architecture in collaboration with IT and software architects, reducing data processing time by 30% while ensuring compliance with enterprise data standards.</code>                 |
* Loss: <code>__main__.LoggingMNRLoss</code> with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim",
      "gather_across_devices": false
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 10
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 10
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `parallelism_config`: None
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch_fused
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `project`: huggingface
- `trackio_space_id`: trackio
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `hub_revision`: None
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: no
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `liger_kernel_config`: None
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: True
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Training Logs
| Epoch  | Step | Training Loss | ft_mpnet_v2_cos_sim_ndcg@10 |
|:------:|:----:|:-------------:|:---------------------------:|
| 1.0    | 311  | -             | 0.6078                      |
| 1.6077 | 500  | 0.4212        | -                           |
| 2.0    | 622  | -             | 0.6312                      |
| 3.0    | 933  | -             | 0.6450                      |
| 3.2154 | 1000 | 0.1801        | -                           |
| 4.0    | 1244 | -             | 0.6429                      |
| 4.8232 | 1500 | 0.0842        | -                           |
| 5.0    | 1555 | -             | 0.6473                      |
| 6.0    | 1866 | -             | 0.6551                      |
| 6.4309 | 2000 | 0.055         | -                           |
| 7.0    | 2177 | -             | 0.6579                      |
| 8.0    | 2488 | -             | 0.6589                      |
| 8.0386 | 2500 | 0.0417        | -                           |
| 9.0    | 2799 | -             | 0.6602                      |
| 9.6463 | 3000 | 0.0273        | -                           |
| 10.0   | 3110 | -             | 0.6639                      |


### Framework Versions
- Python: 3.12.12
- Sentence Transformers: 5.1.2
- Transformers: 4.57.3
- PyTorch: 2.9.0+cu126
- Accelerate: 1.12.0
- Datasets: 4.0.0
- Tokenizers: 0.22.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### LoggingMNRLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->